---
title             : "Exploring variation in infants’ preference for infant-directed speech: Evidence from a multi-site study in Africa"
shorttitle        : "AFRICAN INFANTS’ PREFERENCE FOR INFANT-DIRECTED SPEECH"

header-includes:
  - \usepackage{amsmath}
  - \renewcommand{\topfraction}{1}
  - \renewcommand{\bottomfraction}{1}
  - \renewcommand{\textfraction}{.1}
  - \renewcommand{\floatpagefraction}{1}
  - \setcounter{topnumber}{9}
  - \setcounter{bottomnumber}{9}
  - \setcounter{totalnumber}{20}
  - \setcounter{dbltopnumber}{9}
  
author: 
  - name          : "The ManyBabies Consortium"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Department of Psychology, 450 Serra Mall, Stanford, CA 94305"
    email         : "mcfrank@stanford.edu"

affiliation:
  - id            : "1"
    institution   : "See author note"

author_note: |
  Please address correspondence to XYZ. 

abstract: |
  Infants show a preference for infant-directed speech (IDS) over adult-directed speech (ADS). This preference has been linked to infants’ language processing and word learning in experimental settings, and also correlates with later language outcomes. Recently, the cross-cultural consistency of infants’ IDS preference has been confirmed by large-scale, multisite replication studies, but conclusions from these studies were primarily based on participants from North America and Europe. The current study addressed this sampling bias via a large-scale, multisite study of infants (3-15 months) across XYZ communities in Africa. We investigated whether participants showed a preference for IDS over ADS, and if so, whether the magnitude of their preference differs from effects documented in other populations of infants. DESCRIBE RESULTS HERE


keywords          : "infant-directed speech; reproducibility; Africa; infants; generalizability"
wordcount         : "XYZ"

bibliography      : ["r-references.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_docx
---

<!-- Angeline Sin Mei Tsui (Stanford University, USA) -->
<!-- Alexandra Carstensen (Stanford University, USA) -->
<!-- George Kachergis (Stanford University, USA) -->
<!-- Amina Abubakar (Institute for Human Development, Aga Khan University, Kenya) -->
<!-- Mulat Asnake (Addis Ababa University, Ethiopia) -->
<!-- Oumar Barry (Cheikh Anta Diop University - Dakar, Senegal) -->
<!-- Dana M. Basnight-Brown (United States International University-Africa, Kenya) -->
<!-- Dangkat Bentu (University of Jos, Nigeria) -->
<!-- Christina Bergmann (Max Planck Institute for Psycholinguistics, The Netherlands) -->
<!-- Evans Binan Dami (University of Jos, Nigeria) -->
<!-- Natalie Boll-Avetisyan (University of Potsdam, Germany) -->
<!-- Marguerite de Jongh (Sefako Makgatho Health Sciences University, South Africa) -->
<!-- Yatma Diop (Michigan State University, USA) -->
<!-- Reginald Akuoko Duah (Humboldt-Universität, Berlin and University of Ghana, Legon) -->
<!-- Esther Herrmann (University of Portsmouth, UK) -->
<!-- Chaning Jang (Busara Center for Behavioral Economics, Kenya) -->
<!-- Simon Kizito (Makerere University, Uganda) -->
<!-- Tilinao Lamba (University of Malawi, Chancellor College, Zomba, Malawi) -->
<!-- Limbika Maliwichi-Senganimalunje (University of Malawi, Chancellor College, Zomba, Malawi) -->
<!-- Joyce Marangu (Institute for Human Development, Aga Khan University, Kenya) -->
<!-- Maya Mathur (Stanford University, USA) -->
<!-- Catherine V. Mbagaya (Maseno University, Kenya) -->
<!-- Demeke Mekonnen Mengistie (St. Peter Specialized Hospital, Ethiopia) -->
<!-- Carmen Milton (Sefako Makgatho Health Sciences University, South Africa) -->
<!-- Febronie Mushimiyimana (University Teaching Hospital of Kigali, Rwanda) -->
<!-- Mikateko Ndhambi (Sefako Makgatho Health Sciences University, South Africa) -->
<!-- Irene Ngina (Busara Center for Behavioral Economics, Kenya) -->
<!-- Eunice Njoroge (Institute for Human Development, Aga Khan University, Kenya) -->
<!-- Paul Odhiambo Oburu (Maseno University, Kenya) -->
<!-- Asana Okocha (Princeton University, USA) -->
<!-- Paul Okyere Omane (University of Potsdam, Germany) -->
<!-- Anisha Singh (Busara Center for Behavioral Economics, Kenya) -->
<!-- Andrew S. Ssemata (Makerere University, Uganda) -->
<!-- Juliette Unyuzumutima (University Teaching Hospital of Kigali, Rwanda) -->
<!-- Henriette Zeidler (Aston University, UK & University of Gothenburg, Sweden) -->
<!-- Casey Lew-Williams (Princeton University, USA) -->
<!-- Michael C. Frank (Stanford University, USA) -->


```{r load_packages_settings, include = FALSE}
library("papaja")
library(ggthemes)
library(lme4)
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(ggpubr)


opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
theme_set(theme_bw() + 
            theme(strip.background = element_blank(), 
                  panel.grid = element_blank())) # nice theme with limited extras
```

```{r load_authors}
# # downloaded from https://docs.google.com/spreadsheets/d/1O225ib9wFjUjZJfz9jqwRkfFYaCGXrtDYnmkK9NM3r8/edit#gid=0 on 3/4/19
# author_data <- read_csv(here("metadata/ManyBabies1 Author List and Contributions - Sheet1.csv"), 
#                         skip = 5) %>% 
#   rename(first = `Author First Name`, 
#          last = `Author Last Name`,
#          affil = Affiliation, 
#          grant = `Grant acknowledgment(s)`, 
#          concept = `Early Concept`, 
#          design = `Study Design`,
#          stim = `IDS/ADS Stimuli`, 
#          pilot = `Ran Pilot`,
#          protocol = `Protocol Code/Script`,
#          doc = Documentation, 
#          manage = `Study Management`,
#          data = `Data Collection`,
#          analysis = `Data Analysis`, 
#          ms1 = `Stage 1 Manuscript`, 
#          ms2 = `Stage 2 Manuscript`) %>% 
#   select(first, last, affil, grant, concept:ms2) %>% 
#   mutate(full = paste0(.$first, " ", .$last, " (", .$affil, ")"), 
#          status = case_when(last == "Frank" ~ 1, 
#                             last == "Soderstrom" ~ 3, 
#                             TRUE ~ 2), 
#          initials = paste0(
#            sapply(
#                lapply(
#                  str_split(
#                    gsub("[:a-zé\\öøł:\\.\\ \\-]","",first), 
#                    ""), 
#                  function(x) str_c(x,".")), 
#                function (x) paste(x, collapse =  " ")), 
#            " ",
#            last)) %>%
#   group_by(status) %>% 
#   arrange(last, .by_group = TRUE) 
#          
# authors <- author_data %>% 
#   pull(full) %>% 
#   paste(collapse = ", ")
# 
# grants <- author_data  %>% 
#   filter(!is.na(grant), 
#          grant != "Association for Psychological Science (APS)") %>%
#   filter(!duplicated(grant)) %>%
#   pull(grant) %>% 
#   paste(collapse = "; ")
# # Can't figure out how to render this into YAML, pasting for now.
```


```{r load_labs}
# d_participants <- read_csv(here("processed_data/03_data_trial_main.csv"), 
#                            na = c("NA", "N/A"))
# 
# labs <- d_participants %>%
#   group_by(lab) %>%
#   summarise(n = length(unique(subid)), 
#             method = paste(unique(method), collapse = ", "))
# 
# n <- labs %>%
#   pull(n) %>%
#   sum
# 
# n_labs <- length(unique(labs$lab))
# 
# agegroups_lab <- d_participants %>%
#   filter(!is.na(age_group)) %>%
#   group_by(lab) %>%
#   summarize(n = length(unique(age_group)))
# 
# n_labs_multiple_agegroups <- agegroups_lab %>%
#   filter(n>1) %>%
#   summarize(n=length(n)) %>%
#   pull

```


Adults often speak to infants differently than to other adults, using a speech register known as infant-directed speech (IDS). Infant-directed speech tends to have exaggerated prosodic characteristics, including higher pitch, greater pitch variation, longer pauses, simplified grammatical structure, and shorter and slower utterances as compared to adult-directed speech (ADS; e.g., Fernald et al., 1989, Trainor & Desjardins, 1997). Even very young infants from a variety of language backgrounds have a preference for listening to IDS over ADS (e.g., Cooper & Aslin, 1994; Cooper, Abraham, Berman, & Staska, 1997; Fernald, 1985; Hayashi, Tamekawa, & Kiritani, 2001; Kitamura & Lam, 2009; Newman & Hussain, 2006; Pegg, Werker, & McLeod, 1992; Santesso, Schmidt, & Trainor, 2007; Singh, Morgan, & Best, 2002; Werker & McLeod, 1989). Infants’ preference for IDS over ADS has also been demonstrated in a meta-analysis; across 34 studies, IDS preference had a fairly large average effect size with a value of Cohen’s d 0.72 (Dunst, Gorman & Hamby, 2012) (Bergmann et al., 2018). 

Why do infants prefer IDS? Perhaps IDS is intrinsically salient to infants because of its perceptual characteristics (e.g., higher pitch, greater pitch variability). Or perhaps, as infants are exposed to IDS, familiarity leads to preference. These explanations have different developmental predictions: while the intrinsic view would suggest an early preference (e.g., Cooper & Aslin, 1990), the exposure account would predict developmental increases in preference. Further, these explanations are not mutually exclusive: infants’ early preference for IDS may motivate their parents to use more IDS, which in turn could lead infants to show a stronger IDS preference. 
Regardless of its origins, infants’ preference for IDS may benefit their early language development. For example, in experimental studies, infants can segment words better in fluent speech produced in IDS than ADS (Thiessen, Hill & Saffran 2005), show better recognition of words introduced in IDS after a 24-hour delay (Singh, Nestor, Parikh, Yull, 2009), and more successfully learn words from IDS than ADS (Graf Estes & Hurley, 2013). 

Further evidence comes from correlational studies, which have found that the amount of IDS in the language environment is positively related to children’s language outcomes, such as vocabulary size (e.g., Ramirez-Esparza, Garcia-Sierra and Kuhl, 2014; Shneidman, Arroyo, Levine & Goldin-Meadow, 2013; Shneidman & Goldin-Meadow, 2012; Weisleder & Fernald, 2013; but cf. Casillas, Brown & Levinson, 2020; 2021, who found similar timing of language development milestones even in a population that hears very limited IDS). Together, this work suggests that infants’ preference for IDS over ADS may support their language development, which explains why infants’ IDS preference continues to be an important topic in the literature on early childhood. 

However, it is important to note that almost all prior studies, including the meta-analysis by Dunst and colleagues (2002), have included mainly infants learning English in Western, educated, industrialized, rich, and democratic (WEIRD) societies (Henrich, Heine, & Norenzayan, 2010), with only a few studies extended to non-Western infant populations learning languages other than English (Hayashi et al., 2001; Werker, Pegg, & Mcleod, 1994). As such, there is a large sampling bias in the existing data about infants’ preference for IDS, as in many other research topics in developmental psychology (see Nielsen, Haun, Kärtner, & Legare, 2017). This sampling bias is a problem for generalizing findings about infants’ IDS preference to infants growing up in different cultures and learning different languages. In light of this generalizability issue – as well as the recent replication crisis in psychology (e.g., Open Science Collaboration, 2015) – infant researchers have begun to collaborate on large-scale, multi-site studies to replicate key developmental findings (Frank et al., 2017). 

One of these multi-site projects investigated infants’ preference for IDS over ADS: the ManyBabies1 study (MB1; ManyBabies Consortium, 2020). MB1 collected monolingual data from 67 laboratories, with a total sample of 2329 monolingual infants 3 – 15 months old. The protocol for this experiment was simple: infants listened to alternating audio clips of IDS and ADS while viewing an uninformative visual stimulus (a colored checkerboard). Their looking time was measured over the course of up to 16 trials, 18s each in length (8 IDS and 8 ADS). Notably, all participants in the study listened to stimuli that were constructed from naturalistic speech by North American mothers (speaking either to another adult or to their own infant). The mismatch between the stimuli and the native language of many infants in the study allowed inferences about native language effects and also minimized variability due to differences in the stimuli (a follow-up project now in progress seeks to measure native-language preferences in a subset of MB1 labs). Overall, older infants showed a stronger preference for IDS than younger infants. There was also an effect of infants’ language backgrounds: North American infants exhibited a stronger IDS preference than infants who were not exposed to North American English (NAE). Although infants’ ages and language backgrounds affected the magnitude of IDS preference, essentially all groups of infants preferred NAE IDS over ADS. 

Despite the breadth of its sample relative to previous work, the MB1 study still constitutes a biased sample of infant populations in the world.  Most of the data in MB1 were contributed by laboratories in economically-advantaged areas, accessing relatively high socio-economic status participant populations. Further, although this large-scale study had a diverse sample from 17 countries, 60 out of the 67 participating laboratories were from Europe and North America, only a handful of laboratories were from Australia and Asia, and none were from Africa or South America. Thus, the sample studied in MB1 came almost exclusively from Western, educated, affluent populations who heard Indo-European languages, limiting the generalizability of the findings to infants growing up in other cultural and linguistic contexts. This lack of evidence on generalizability of a key finding about infants’ preference restricts our ability to build robust developmental theories of language learning across cultural contexts. Our current study takes a step towards addressing this gap. 

We investigate whether infants growing up in a variety of African cultures show an IDS preference, using the paradigm developed by the MB1 study. Our study has both a theoretical goal and a practical goal. Theoretically, we are interested in whether IDS preference is a culturally and linguistically invariant developmental pattern (Neilson et al., 2017). The inclusion of infants across many African cultures (who are acquiring many different languages, see Table 1) provides an important test of generalizability of the IDS preference. Practically, increasing sample diversity also promotes diversity among researchers engaged in developmental science and hopefully increasing exchanges between researchers across cultures. Thus, one goal of our study is building research networks to facilitate further studies with the communities represented in the current study.

Our study builds on a foundation of prior descriptive work investigating the generality of IDS across cultures. Although this work has investigated a variety of different cultures and languages, it can be (and often is) crudely summarized via the distinction between WEIRD and non-WEIRD cultures discussed above. We follow this convention here without endorsing this distinction as necessarily being meaningful in the context of our study, as IDS in WEIRD and non-WEIRD cultures shares similar prosodic properties. For example, Broesch and Bryant (2015) reported that IDS produced by North-American mothers, as well as by Kenyan and Fijian mothers, is produced with higher pitch, greater pitch variation, and is spoken at a slower rate than ADS. This finding is consistent with past work reporting that IDS shares some common exaggerated prosodic features (e.g., higher pitch, larger pitch variation) across diverse languages, which include French, Italian, German, Japanese, British English, American English (Fernald et al., 1989), Mandarin Chinese (Grieser & Kuhl, 1988), Thai, Australian English (Kitamura et al., 2001), Arabic (Farran, Lee, Yoo & Oller, 2016). 

IDS can also be recognized as being infant-directed by listeners from non-WEIRD cultures. Bryant, Liénard and Barrett (2012) reported that Turkanan adults in Kenya can discriminate between NAE IDS and ADS (see similar results in Bryant & Barrett, 2007 for Shuar hunter horticulturists from Amazonian Ecuador). These studies are consistent with findings from the MB1 studies showing that children who are not learning NAE, including children from Singapore and Korea, nonetheless show a preference for NAE IDS over ADS. Taken together, the common acoustic properties of IDS across different languages, and how NAE IDS can be recognized by non-native participants, raise the possibility of infants’ IDS preference over ADS being quite consistent across different cultures and languages. However, it is possible that the strength of this preference will nonetheless be influenced by similarity between the test language (English) and the language(s) that each infant is learning, which could bolster the measured preferences to the extent that test and native language are similar (as in the case of infants learning other Indo-European languages with similar phonetic and acoustic properties). If this is the case, we expect that phylogenetic similarity between Indo-European languages and our stimuli would lead to comparable or stronger observed IDS preferences in samples of infants learning Indo-European languages than those learning languages in other families (e.g., Bantu, the language family we expect to be most prevalent in our sample).  

Despite evidence for general recognition of and preference for IDS across cultures, the strength of IDS preferences is likely modulated by exposure. Exposure to IDS in the home environment varies widely both within and between cultures (Casillas et al., 2020; 2021; Cristia, Dupoux, Gurven & Stieglitz, 2017; LeVine et al., 1994; Shneidman & Goldin-Meadow, 2012; Vogt, Mastin, & Schots, 2015). Differences in IDS quantity have also been hypothesized to reflect differences in child-rearing practices across cultures. For example, direct verbal interaction between parents and infants can be rare in some societies (Heath, 1983; LeVine et al., 1994; Shneidman & Goldin-Meadow, 2012; Weber, Fernald, & Diop, 2017; LeVine & LeVine, 2016). Children in these societies – which are typically non-WEIRD, though certainly not all non-WEIRD societies can be characterized this way – are often expected to learn through observation and participation according to their skill levels (see Legare, 2019 for a review). Thus, infants and young children in such societies may hear less IDS directly from their caregivers than those in WEIRD societies in which the norm involves a greater degree of direct address to parents. Of course, variation is also present within as well as across cultures. Within-culture variation has primarily been studied in North American contexts, where children from higher socioeconomic status (SES) families tend to hear more IDS than children from lower SES families (e.g., Hart & Risley, 1995; Hoff, 2003; Huttenlocher, Waterfall, Vasilyeva, Vevea & Hedges, 2010; Rowe, 2012; Shneidman & Goldin-Meadow, 2012; Weisleder & Fernald, 2013).  

By virtue of our broad sample of African cultures, we expect that our study will likely capture substantial cultural variation in the average amount of IDS in children’s environments. The African sites we sample vary widely in their degree of urbanization, their culture, their parenting values, and the average resources available in children’s home environments – all of which have been argued to be meaningful dimensions governing children’s early linguistic environment. For example, Keller (2012) suggested three prototypical cultural environments for children based on the degree of urbanization of the families in Western and non-Western societies. In this framework, in Western middle-class urban societies, highly educated parents generally aim to help children develop individual psychological autonomy. In contrast, in non-Western rural subsistence-based societies, parents generally aim to help children develop communal action autonomy, so that children have a strong sense of social responsibility and can contribute to the economic functioning of the family (e.g., farming). Importantly, non-Western middle-class urban societies are a hybrid of non-Western, rural and Western, urban societies, where parents generally want their children to develop more individual autonomy but also emphasize the importance of social responsibility in a large family. Broadly speaking, we expect that the African families in our study will be from the non-Western, urban and non-Western, rural groups in this taxonomy (see Table 1). 

The confirmatory analyses of our study are designed to test whether there are differences in the magnitude of IDS preferences measured in this sample and in the prior samples of MB1. Although the average IDS production in the African sites we examine is unknown, consistent differences along this dimension might plausibly lead to variation in the magnitude of IDS preferences between our current study and MB1. In addition, our exploratory analyses will attempt to understand whether variation in IDS preference among infants in our sample of African cultures is explained by demographic proxies related to this taxonomy (e.g., urbanization and/or socioeconomic status). Finally, we will use an exploratory measure of subjective IDS use as a proxy of IDS quantity within families to probe links between parent reported IDS use and infant preference. 

Since multilingualism is common in Africa (e.g., Posel & Zeller, 2016; Rosenhouse & Goral, 2008), many African children begin learning two or more different languages during infancy. Will early multilingualism alter infants’ preferences for IDS? The ManyBabies1-Bilingual (MB1B) study provides some evidence that bilingual infants showed a similar preference for NAE IDS when compared to monolingual infants (Byers-Heinlein et al., in press). MB1B examined bilingual infants’ preference for NAE IDS at 6 to 9 months and 12 to 15 months and found that bilingual and monolingual infants did not differ in terms of the magnitude of their IDS preferences. MB1B also found similar results to MB1, that older bilingual infants and those bilinguals with higher exposure to NAE show stronger IDS preference. However, as in the MB1 study, data collected in MB1B mainly came from laboratories in WEIRD areas, such as North America and Europe, with no laboratories from Africa, so the same caveats of generalizability apply to MB1B as to MB1. Thus, in the current study, we will include both monolingual and multilingual infants, allowing us to assess the generalizability of MB1B’s conclusions to our samples in Africa. 

In sum, there are three primary (confirmatory) goals for the current study. First, we aim to measure infants’ preference for North-American English IDS across a range of cultural and linguistic contexts in Africa. Second, we seek to measure developmental changes in this preference. As we found that older infants show stronger IDS preferences than younger infants in both MB1 and MB1B, we will evaluate whether participants in our study show the same developmental increases in IDS preference. Finally, we will investigate whether there are differences in IDS preferences between infants in Africa in our study and those in Europe and Asia in MB1 and MB1B. As an exploratory aim, we also will examine relationships between parents’ demographics, their responses to survey items regarding subjective use of IDS, and their child’s IDS preference. 

# Methods

## Participation Details

### Time-frame

On July 23, 2018, we issued an open call for participation by African researchers via listservs and social/professional networks. In total, XYZ laboratories agreed to participate (See Table 1 for target sample characteristics of each site). Our participating laboratories will recruit infants living in eastern (e.g., Kenya), western (e.g., Senegal) and southern (e.g., South Africa) regions of Africa. We also note that many of our participating laboratories are located in East Africa, thus East African participants are disproportionately represented in our sample.  Given the current state of the COVID-19 outbreak, all of our participating sites are currently under lockdown. So, we plan to start data collection as soon as the outbreak is controlled, and the participating sites reopen. We expect data collection to be finished after 12 months. Data collection began on DATE and ended one year later, on DATE. 
Further, we expect to complete data analysis and write up the whole manuscript for Stage 2 submission 4 months after data collection.

### Age distribution

Each participating laboratory was asked to recruit participants in two age bins: 3;0 – 9;0 and 9;1 – 15;0 months. Similar to MB1, each laboratory was asked to collect data spanning the age bin window, but aiming for the mean of the age bin. 

### Sample size determination

We estimated the effect size of infants’ IDS preference on the basis of the data from MB1. We used data from laboratories in MB1 that used the single-screen central visual-fixation preference procedure (which we also use here: see below) and that tested infants with no exposure to North American English (similar to our population of interest). In a mixed-effects model, we examined the effect of test trial type (IDS vs. ADS) on infants’ looking time (log-transformed seconds), while controlling for normally-distributed random intercepts by infant and laboratory. The intercept, representing infants’ average log-looking time across ADS trials, was 1.91; the variances of the random intercepts were 0.074 and 0.022 at the infant and laboratory levels respectively. The fixed-effect coefficient representing infants’ preference for IDS over ADS was 0.080 and the residual variance was 0.33. 

In the first power analysis, we simulated datasets based on the above coefficient estimates and variances. Using the simr package in R (Green & MacLeod, 2016), we ran a power analysis for a mixed-effect analysis with the above-mentioned simulated datasets (number of simulations = 1000). We were uncertain exactly how many labs to assume but settled on 10, given the likelihood of some later signups as well as some lab attrition. Assuming that we had 240 infants across 10 laboratories in each simulated dataset and an alpha level of 0.05, we found that the average power was 99.40% [95% confidence interval: 98.70% – 99.78%] to detect the fixed ADS vs. IDS coefficient of 0.08. This first power analysis was based on very small random-effect variances estimated from MB1 and MB1B datasets. Given that most of the laboratories that participated in MB1 and MB1B had more resources and more extensive experience in running infancy studies in comparison to the participating laboratories in Africa, we planned for potentially higher variances in the data collected in the current project. Thus, we ran a conservative second power analysis by doubling the values of the random intercept and residual variances reported in the datasets from MB1 and MB1B, while holding constant the intercept and the fixed-effect coefficient representing infants’ preference for IDS over ADS. With larger variances, the average power estimate dropped to 87.20% [95% confidence interval: 84.97% – 89.21%] for a total sample of 240 infants. The power analysis can be found at https://osf.io/jgr79/?view_only=5ee43f58762742daaa2caa21b85e3780. 

Prior to submission of the Stage 1 report, we had 11 laboratories committed to collecting data for this project. Given that MB1 reported around 15% data excluded in the final analysis, we expect the exclusion rate for our project is around 15% to 20%. Thus, each laboratory agreed to contribute a minimum of 32 infants (16 infants in each age bin), including infants tested but excluded for reasons not related to the demographic and age inclusion criteria (e.g., fussiness). Further, we encouraged each laboratory to contribute additional data beyond that minimum.  We propose that our projected sample size of 352 would have sufficient power, as 80% of this sample size exceeds our targeted final sample size (n = 240) based on the power analysis described above. 

### Ethics

All laboratories collected data under their own independent IRB protocol. Videos of individual infant participants during the experiment were recorded and stored at each laboratory. However, these videos were not shared with the central data analysis team. Laboratories were instead asked to only submit de-identified data for central data analyses. 

## Exclusion Criteria

All data collected for the study (i.e., every infant for whom a data file was generated, regardless of how many trials were completed) were uploaded to a central database for data analysis. Every laboratory followed the protocol to report any infants who were tested in this study, including those who were excluded from the analysis. Furthermore, each laboratory followed the protocol to make note of the reasons that infants were excluded from the study. 

Typically, participants were only included in the analysis if they met all of the criteria below. However, we allowed parents to choose not to answer some of the questions (e.g., about full-term gestation and developmental disorders) because disclosures might violate cultural norms in some areas of Africa. Thus, participating laboratories may have included infants who did not fully meet the inclusion criteria defined here:

### Full-term

We defined full term as gestation times greater than or equal to 37 weeks. XYZ (%XYZ) of infants tested did not meet this criterion, and were excluded from further analysis. To maximize parents’ comfort in participating in the experiment, they were given the option of not responding to questions about gestation.

### No developmental disorders or hearing loss

We excluded infants with parent-reported developmental disorders (e.g., chromosomal abnormalities, etc.) or diagnosed hearing impairments. Developmental disorders and delays are stigmatized in some cultures in Africa (e.g., negative attitudes towards children with disorders or delays), therefore some parents may decline to answer the question about children’s developmental disorders. In this case, we still tested the infants and included the infants’ data in the analysis. This inclusion criterion was chosen to allow us to retain as much data as possible while ensuring our questionnaire accommodates cultural norms. Further, we noted that only 2 participants (i.e. less than 0.1%) in MB1 were excluded based on parents’ report of developmental disorders. Accordingly, we do not expect that including children whose parents decline to answer this question will lead to an inclusion of large numbers of children with developmental disorders that could potentially skew the results in the study. XYZ (%XYZ) of the infants tested did not meet this criterion. (We did not plan exclusions based on self-reported ear infections unless parents reported medically-confirmed hearing loss.)

### Trial-level and session-level errors

Following MB1 and MB1B, we adopted a relatively liberal inclusion criterion for this study. To be included in the study, a child must have contributed non-zero looking time on at least one pair of test trials (i.e., one trial each of IDS and ADS from a particular stimulus pair). We asked laboratories to identify two different types of errors when uploading their data: trial-level errors and session-level errors. Trial-level exclusions were based on whether we could use infants’ data from a particular test trial. For example, if an infant only completed the first six test trials of the experiment, we entered this infant’s data from the first six trials and discarded data from all other trials. In this case, laboratories would identify this infant’s data from the first to sixth trials as “no trial errors” and any trials from the seventh trial onwards would be identified as “trial errors”. In contrast, session-level errors were errors that occurred when running a particular participant. This type of error is different from the trial-level error exclusions because it indicates that errors occurred which affected an entire session (e.g., failure to save data in the experiment). If a laboratory indicated a session-level error for a particular infant, all data from this infant was excluded from the analysis. In sum, infants who can contribute at least one pair of test trials (i.e., one IDS trial and one ADS trial) would have some data excluded at the trial level whereas infants who cannot contribute one pair of test trials would be excluded at the session level. In general, errors included the following: equipment error (e.g., no sound or visuals on the first pair of trials), experimenter error (e.g., an experimenter was unblinded in setups where infant looking was measured by live button press), or evidence of parent interference or other types of interference (e.g., talking or pointing by parents, construction noise, sibling pounding on door), and infants being uncooperative or fussy (e.g., crying, not willing to do the experiment). Overall, for trial-level exclusions, XYZ trials (%XYZ of all trials) were excluded. For session-level exclusions, XYZ (XYZ% in the final sample) infants were dropped from analysis due to session-level errors.

## Participants

### Final sample

Our final sample included XYZ infants (XYZ% female; see Table XYZ for more specific sample demographic information) from XYZ laboratories (mean sample size per laboratory: XYZ, SD: XYZ, range: XYZ – XYZ). The mean age of infants included in the study was XYZ days (range: XYZ – XYZ). There were XYZ infants in the 3- to 9-month-old bin, XYZ infants in the 9- to 15-month-old bin. An additional XYZ infants were tested but excluded (see the full details on exclusions above). 
	
As mentioned in the Introduction, multilingualism is common in Africa. Thus, many infants in the final sample are likely to have been exposed to more than one language. To assess infants’ language backgrounds, each laboratory completed a family questionnaire with the participating parents (see materials in linked repository: https://osf.io/jgr79/?view_only=5ee43f58762742daaa2caa21b85e3780). Our family language background questionnaire was created based on the family language background questionnaire in the MB1 and MB1B studies, and included questions asking parents to estimate the number of hours that their infants heard different languages. We calculated the percentage of time that infants were exposed to a given language as the number of hours they hear that language (per day) divided by the total number of hours the infant hears any language each day. This method is simpler than the traditional interview method used in assessing bilingual infants’ language exposure (Byers-Heinlein et al., 2019), but in order to minimize the burden on participating laboratories and families, we decided to use a short questionnaire method to assess infants’ language backgrounds. 

In this paper, we define bilingualism following the criteria established in MB1B (Byers-Heinlein et al., 2021). Monolingual infants are defined as those who have a minimum of 90% exposure to one language. Simultaneous bilingual infants are defined using the following criteria: (i) infants are regularly exposed to two or more languages beginning within the first month of life; (ii) they have a minimum of 25% exposure to each of their languages. In other words, bilingual infants are exposed to two languages between 25% to 75% of their time. Based on these criteria, it is possible that bilingual infants in our paper were exposed to multiple languages. For example, an infant with 45% English, 45% French, and 10% Spanish exposure would be regarded as a bilingual infant. Infants who did not meet the bilingual or monolingual criteria were designated as “other language background.” All infants were included in the main, confirmatory analyses regardless of language background. Language background groupings were treated as a covariate in the analyses. 

Based on the above-mentioned criteria, XYZ infants were classified as monolingual infants, XYZ infants were classified as bilingual infants, and XYZ infants were classified as other.  

## Materials

Visual stimuli. All visual stimuli were the same as those used in the MB1 study. We used a brightly colored static checkerboard as the fixation stimulus, and an animation with shrinking concentric multi-colored circles to ensure infants were attending to the screen at the start of each trial. All of the stimuli can be found at https://osf.io/wh7md/. 

Auditory stimuli. All auditory stimuli were identical to those used in the MB1 study. The stimuli were recordings of North-American English mothers either speaking with experimenters (ADS) or with their infants whose ages ranged from 122 to 250 days in a laboratory setting. Mothers were provided with a set of objects and were asked to talk about the objects with the experimenters and their infants in separate recording sessions. In total, two sets of auditory stimuli were created: one set consisted of 8 IDS stimuli and the other set consisted of 8 ADS stimuli. Each stimulus lasted for 18 seconds. The details of stimulus creation can be found in the report of MB1 (ManyBabies Consortium, 2020). 

Volume. Each laboratory  measured stimulus volume level using a smartphone app (e.g., the Android app “Sound Meter”). Labs kept the stimulus volume close to 63 – 65 dB SPL. According to the protocol, labs would measure and report the background noise level and the stimulus level. XYZ labs provided these data. The average background level was XYZ dB SPL (SD: XYZ, range: XYZ – XYZ) and the average stimulus level was XYZ dB SPL (SD: XYZ, range: XYZ – XYZ). 

## Procedure

Apparatus. Each laboratory used a laptop computer that had the experiment programmed in Habit 2.26 (Oakes, Sperka, DeBolt & Cantrell, 2019). Moreover, each laboratory used a computer monitor to present the visual stimuli, a speaker for audio stimuli, a webcam for the experimenter to observe and record infants’ performance, curtains/room dividers that separated the experimenter from the infant and parent during the experiment, and two sets of headphones: one for the experimenter and one for the parent. 

Experimental procedure. The procedure was identical to the single-screen central visual fixation preference procedure reported in the MB1 study (ManyBabies Consortium, 2020). Using the single-screen central fixation method, researchers measured in real time the duration of infants’ looking time to the computer monitor while they listened to the audio recordings. Infants’ looking time to the computer monitor indicated their preference for the audio recordings (i.e., IDS/ADS). Each laboratory followed procedural instructions closely (based on pre-recorded videos illustrating the procedures, which were shared with all participating laboratories) to maintain the consistency of the experimental procedure across laboratories. 

The experimenter explained the study to the parent and obtained consent from the parent before running the experiment. After completing the consent form, the experimenter led the participant to the testing room. To minimize distraction, the experimenter was separated from the infant and parent by curtains or a room divider. During the experiment, the infant sat on the parent’s lap. To minimize any bias introduced by the experimenter or parent hearing the stimuli, each of them wore headphones and heard masking music during the experiment. 

Parents were instructed not to speak to the infant during the experiment and not to point to the screen. Infants' performance was recorded by a webcam that was placed in front of and below the computer monitor. Infants' looking time to each trial was measured online by the experimenter, who observed the infant’s behavior via the webcam. At the beginning of each trial, a short video of a colorful circle was presented to orient the infant's attention to the screen. Once the infant fixated on the screen, the experimenter started the trial. The first two trials of the session were warm-up trials that accustomed infants with the procedure of the experiment, so the infant’s looking time during warm-up trials was not analyzed. The auditory stimuli for the warm-up trials was piano music that lasted 18 seconds on each trial and the visual stimulus was the same as in the test trials (i.e., a colorful checkerboard). After the first two warm-up trials, the infant was tested with 16 trials presenting the IDS and ADS stimuli. Each infant was randomly assigned to one of four pseudo-random orders to counterbalance the order of presentation of IDS and ADS stimuli. Within each order, there were four blocks and each block presented 2 IDS and 2 ADS trials in alternating order. The presentation of the trials within each block were counterbalanced such that two blocks started with an IDS trial, and the other two blocks started with an ADS trial. On each trial, the auditory stimulus would continue to play until the infant looked away for 2 consecutive seconds or reached the maximum length of the auditory stimulus (18 seconds). Experimenters used the Habit program to record all looking time for every trial. There was no minimum looking time per trial that was required for continuation of the experiment. However, as in the MB1 study, any looking time that was less than 2 seconds was not analyzed. We excluded XYZ (XYZ%) trials that had less than 2 seconds looking time in total. 

After the main looking-time task, the parents answered questions from the experimenter about participant and family demographic information, such as infant sex, date of birth, language exposure, and preterm/full term status. The questionnaire was translated into the appropriate language(s) for participants from each data collection site. See supplementary materials for the English template and adaptations. 

## General Lab Practices

### Training of the experimenters

Three of the authors conducted a 2-day training workshop in Nairobi, Kenya on January 28 – 29, 2020, which was attended by lead researchers from 8 of the participating laboratories. The training session provided an overview of the experimental procedure, advice on setting up the apparatus at the researcher’s institution, and training, instructions and guidelines for running the experiment. Further, the first author sent instructions for experiment set-up and the workshop materials to all participating laboratories, and kept close contact with all lead researchers in the participating laboratories to provide technical support for the experiment. 

### Training of research assistants

Each laboratory was responsible for maintaining good experimenter training practices. We extended an invitation for the training workshop to one research assistant in each laboratory, so that the researcher primarily responsible for data collection could receive training directly as well. Following the MB1 study, each laboratory reported on which research assistant ran each infant using pseudonyms or numerical codes. After data collection, each laboratory completed a questionnaire regarding their training practices, the experience and academic status of each experimenter, and their basic participant greeting practices. 

# Results

ALL PLANNED ANALYSES ARE DETAILED BELOW IN OUR PRE-SUBMISSION MS. 

## Confirmatory Analyses

Data processing and analytic framework. Our primary dependent variable of interest was infants’ looking time (LT). Infants’ looking time was defined as time spent fixating on the computer screen during test trials. We did not count LT when infants looked away from the screen, though the trial was discontinued if an infant looked away and did not look back to the screen within 2 seconds. Following MB1 and MB1B, we log-transformed looking times prior to statistical analysis (Csibra, Hernik, Mascaro, Tatone, & Lengyel, 2016). We made this decision because we wanted to compare the data of the current study with those in MB1 and MB1B. 

We tested our research questions via general linear mixed effects models. We fit all models using a maximal random effects structure (Barr, Levy, Scheepers, & Tily, 2013). Under this approach, we first specified all random effects that are appropriate for the experimental design (e.g., IDS/ADS trial type varied within subjects in our experimental design, thus it can be specified as a random effect by subject; see below for the full list of effects considered). If any of these mixed-effects models failed to converge, we used an iterative pruning strategy: first removing random slopes nested within subjects, next removing random slopes nested within labs, and finally removing random intercepts from groupings in the same order, retaining effects of trial type as these were of greatest theoretical interest. Following MB1 and MB1B, we fit all models using the lme4 package with the bobyqa optimizer, version XYZ (Bates, Maechler, Bolker, & Walker, 2015) and computed confidence intervals and p values using the lmerTest package (Kuznetsova, Brockhoff, & Christensen, 2017). 

In addition to the mixed-effect models, we assessed the reliability of measurement in our study by reporting the reliability of the infants’ looking time difference to the IDS vs ADS stimuli across different trials. Following Byers-Heinelin et al. (under review), we reported the intraclass correlation coefficient (ICC) as our reliability measure. The ICC was computed using the psych package in R (Reville, 2018). We reported an ICC3k measure, on the basis of a two-way random effects model, a mean-rating of 8 (i.e., we had 8 pairs of IDS and ADS trials) and consistency agreement (Koo & Mae, 2016, Parson et al., 2019). The estimated ICC was _____, 95% CI [____, ____]. We will compare the estimated ICC in our study with the ICC in MB1(B); we expect that the ICC between our study and that in MB1(B) to be similar in magnitude.

Below is a description of variables in our mixed-effect models:

* Log_lt:  Dependent variable. Log-transformed looking time in seconds.
* Trial_type: a dummy coded variable with two levels: ADS (reference) and IDS. A positive coefficient means that infants look longer to IDS trials compared to ADS trials. 
* Age_months: a continuous variable measuring the infant’s age in months (centered). 
* Trial_num: An index for the current trial (1-16 for infants who completed the experiment). Excluded trials were reflected as missing trial numbers.
* Language_background: this consisted of two dummy coded variables that represented infants from three different language backgrounds: monolinguals ( >= 90% exposure to one’s native language); bilinguals ( >=25 % to each of their languages); other (any infants who were not categorized as monolinguals or bilinguals). Using monolinguals as the reference level, the two dummy-coded variables are: (i) bilingual – infants who were categorized as bilinguals would be coded as 1 and all other infants would be coded as 0; (ii) Other (any infants who are not monolinguals or bilinguals) – infants who were categorized as other would be coded as 1 and all other infants would be coded as 0. In this case, monolingual infants would be coded as 0 in the above-mentioned dummy-coded variables.
* Infant_ID: a dummy coded variable with two levels, representing infants living in Africa in our current study (coded as 1) and infants living in Europe, Australia and Asia who were not hearing North American English, with data from MB1(B) (coded as 0). 

As a reminder, we examined the following research questions in our paper: (1) IDS preference: whether infants in our multi-site African sample showed a preference for IDS and what is the corresponding effect size of this preference; (2) Age effect: whether there were changes in the infants’ IDS preference across different ages; (3) Population comparison: examine whether the magnitude of infants’ IDS preference in our study differed from infants in MB1 and in MB1B (comparing only infants in these three samples who were not exposed to North American English). 

Research questions 1 and 2: Infants’ IDS preference and age effect. We addressed our first two research questions using only the data collected in the current paper from laboratories in Africa. We specified the following model: 
log_lt ~ trial_type + trial_num + age_months + 
trial_type * trial_num + 
age_months * trial_num + 
age_months * trial_type + 
(trial_type * trial_num | subid) +
(trial_type | lab) 

The fixed-effects structure of this model included main effects of trial type (IDS vs ADS),  age, and trial number. This structure controls for the effects of each independent variable on infants’ looking time (e.g., longer looking times for IDS, shorter looking times on later trials). In addition, we included several two-way interaction terms: trial type interacting with trial number to model the possibility of infants’ faster habituation to ADS, age interacting with the trial type to model the developmental trajectory of infants’ IDS preference, and age interacting with trial number to model faster habituation for older children. The random effects structure of the model controlled for subject-level and lab-level grouping. For subject-level grouping, we added random intercepts and random effects of trial type, trial number, and their interaction to model the possibility that each infant may have different rates of habituation for IDS and ADS trials. For lab-level grouping, we added a random effect trial type to model differences in IDS preferences across labs.  

After pruning for non-convergence, our final model specification was: To be filled in after data analysis. 

As in MB1 and MB1B, the fixed effect estimate for trial type corresponds to the predicted infant-directed speech preference effect in units of log looking time (research question 1). The fixed effect estimate for the interaction of trial type and age indicates the estimated age-related change in infant-directed speech preference in log seconds per month (research question 2). 

We will report metrics that directly describe the heterogeneous distribution of effects across labs (Mathur & VanderWeele, in press; Mathur & VanderWeele, 2019). Specifically, we will use the R package MetaUtility to estimate (Mathur & VanderWeele, 2020): (1) the percentage of true population effects (i.e, infants’ preference for IDS over ADS) across labs that are greater than 0; (2) the percentage that are greater than an effect size of by Cohen’s d = 0.2; and (3) the percentage of effects in the unexpected direction that are less than d = -0.2 (i.e., representing infants preferring ADS over IDS). We will present these metrics only if the random slopes of trial type by lab are included in the final pruned mixed model, if these random slopes appear heterogeneous across labs (i.e., their estimated variance is greater than 0), and if, as anticipated, at least 10 labs contribute data (i.e., to achieve valid statistical inference).

Research question 3: Population comparison. In this analysis, we compare the data collected from the laboratories in Africa to data collected in MB1 and MB1B in COUNTRIES outside North America. We selected the subset of data from MB1 and MB1B that was collected using central fixation procedures (to match methods across studies) and from infants who were not exposed to North American English (non NAE) (to match stimulus un-familiarity due to language background). While we could have controlled the methodological and demographic variables statistically (and hence included all data from MB1 and MB1B in the full model), we believed that the increase in model complexity – and comparable decrease in interpretability – outweighed the benefits of this strategy. 

We examine whether our sample of infants’ IDS preference is different from those in MB1 and MB1B with the following model:
log_lt ~ trial_type + trial_num + age_months + infant_ID + language_background +
trial_type * trial_num + 
age_months * trial_num + 
age_months * trial_type + 
trial_type * infant_ID +
trial_num * infant_ID + 
trial_type * language_background +
(trial_type * trial_num | subid) +
(trial_type | lab) 

In this mixed-effects model, the fixed-effects included main effects of trial type, language background, age, trial number, infants in our study/non NAE infants in MB1(B) and language background. In addition, we included several two-way interaction terms in the fixed effects structure: (i) trial type interacted with trial number, modeling the possibility of infants’ faster habituation to ADS, (ii) age interacted with trial number, modeling faster habituation for older children, (iii) age interacted with trial type, modeling the developmental trajectory of infants’ IDS preference, (iv) trial type interacted with infants in our sample, modeling the possible difference in IDS preference between infants in Africa and infants tested in MB1 and MB1B, (v) trial num interacted with infants in our sample, modeling the possible difference in habituation between our sample of infants and infants tested in MB1 and MB1B, and (vi) trial type interacted with language background, modeling the possible difference in IDS preference from infants with different language backgrounds. We adopted the same baseline random effects as in the previous model. 

After pruning for non-convergence, our final model specification was: To be filled in after data analysis. The fixed effect estimate corresponding to our research question is the trial_type * infant_ID, which captures differences in measured IDS preference between the current data and data from MB1/MB1B in units of log seconds of looking time. 
## Exploratory Analyses

TO BE WRITTEN AFTER THE STUDY IS COMPLETED

### Urban vs rural areas
	
Prior studies (e.g., Keller, 2012; Vogh et al., 2015) have found that parents in non-WEIRD contexts sometimes speak to their infants differently across urban and rural areas. For example, parents in urban areas of Mozambican communities in Southeastern Africa tend to speak more to their children relative to parents in rural areas of Mozambican communities (Vogh et al., 2015). In turn, this could potentially lead to differences in IDS preference, with infants from urban areas, because of their higher input, showing a larger IDS preference, compared to infants from rural areas. We plan to explore this possibility by examining whether possible demographic differences in language input affect infants’ preference for IDS in our sample.
	
### Socio-economics status (SES)

Previous research in North America (e.g., Hart & Risley, 1995; Hoff, 2006; Weisleder & Fernald, 2013) has shown that the quantity and quality of child-directed speech vary across families with different SES backgrounds. These differences in language input may drive differences in infants’ preference for IDS. Thus, we plan to explore how SES affects infants’ preference for IDS. SES will be measured by mothers’ formal education (number of years), and the MacArthur Scale of Subjective Social Status (MacSSS). We will enter both mothers’ formal education and the MacSSS as two separate SES variables in the regression model, after checking the assumption about the collinearity between variables. We note that SES is likely to be positively correlated with whether the family lives in an urban area. However, we propose that our measure of SES most likely can provide more information about the demographic backgrounds of the families in addition to the binary measure of whether the family lives in an urban or rural setting. Thus, this analysis may be more fine-grained, and could allow us to detect differences in infants’ IDS preference across different demographic contexts.
	
# General Discussion

TO BE WRITTEN AFTER THE STUDY IS COMPLETED

We summarize our findings with respect to three research questions in the paper.
What is the magnitude of the IDS preference from our sample of infants? We found a magnitude of XYZ. 

Does IDS preference vary across age in our sample? We found XYZ...

Is there any difference in IDS preference between infants in our sample and those in previous MB samples? We found XYZ...

	The general discussion will include caveats around over-interpretation of any demographic differences in IDS preference given that IDS preference has not been shown to be individually predictive of any later outcomes (see MB1-CDI extension; Soderstrom et al., in prep). 








<!-- # Methods -->

<!-- <!-- ALL METHODS ARE DETAILED BELOW IN OUR PRE-SUBMISSION MS, BUT SOME DETAILED METHODS MAY APPEAR IN AN ONLINE SUPPLEMENT RATHER THAN THE MAIN TEXT. --> -->

<!-- ## Participation Details -->

<!-- ### Time frame -->

<!-- We issued an open call for labs to participate on February 2nd, 2017. Data collection began on May 1st, 2017. Data collection was scheduled to end on April 30th, 2018 (one year later). In order to allow labs to complete their sample, however, a 45 day extension was granted, and data collection officially ended on June 15th, 2018. Data collection from one laboratory extended beyond this timeframe (see below in Methods Addendum). -->

<!-- ### Age distribution  -->

<!-- Each participating lab was asked to recruit participants in one or more of four age bins: 3;0 - 6;0, 6;1 - 9;0, 9;1 - 12;0, and/or 12;1 - 15;0 months. Each lab was tasked with ensuring that, for each age bin they contributed, the mean age fell close to the middle of the range and the sample was distributed across the bin. We selected three-month bins as a compromise, on the assumption that tighter bins would make recruitment more difficult while broader bins would lead to more variability and would blur developmental trends (i.e., by introducing possible interactions between age and lab-specific effects, for instance, if a particular method turned out to be most appropriate for a subset of the ages tested). This flexibility was necessary because labs differ in their ability to recruit infants of different ages. -->

<!-- ### Lab participation criterion  -->

<!-- During study planning, we used data from MetaLab [@bergmann2018promoting] to compute  the meta-analytic mean effect size for IDS preference; the resulting value was  Cohen’s *d* = .72. In a paired *t*-test, 95% power to detect this effect requires 27 participants, and 80% power requires 17. On the basis of these calculations, we asked participating labs to commit to samples with a minimum of $N = 32$ in a single age group. However, given that for many of our analyses, power across labs is more critical than within a lab [@judd2017experiments], we allowed labs to contribute a “half sample” of $N = 16$, with the assumption that this would increase the number of laboratories capable of participating and allow more laboratories to contribute samples from multiple age bins. We specified that labs should recruit with respect to the desired demographic characteristics of the study (e.g., full-term infants; see below for full list of exclusion criteria). Given this recruitment strategy, however, we asked that sample *N*s be calculated on the basis of the number of total infants tested, not the infants retained after exclusions (which were performed centrally as part of the broader data analysis, not at the lab level).  -->

<!-- <!-- Note github issue #138 for interpretation of this point. --> -->

<!-- We included data from a lab in our analysis if they were able to achieve the minimum *N* required for a half-sample in their age bin ($N=16$) by the end date of testing and if, after exclusions, they contributed 10 or more data points. If a lab collected more than their required sample, we included the extra data as well. Laboratories were cautioned not to consider the data (e.g., whether a statistically significant effect was evident) in their lab internal decision-making regarding how many infants to recruit/when to stop recruitment. 	 -->



<!-- ## Participants -->

<!-- ```{r} -->
<!-- excluded_labs <- read_csv(here("processed_data","02_validated_output.csv"),  -->
<!--                           na = c("NA", "N/A")) %>% -->
<!--   pull(lab) %>% -->
<!--   unique %>% -->
<!--   setdiff(unique(d_participants$lab)) %>% -->
<!--   length -->
<!-- ``` -->

<!-- Our final sample was comprised of `r n` monolingual infants from `r n_labs` labs (mean sample size per lab: `r round(mean(labs$n), 2)`, $SD= `r round(sd(labs$n), 2)`$, range: `r round(min(labs$n), 2)` -- `r round(max(labs$n), 2)`; `r n_labs_multiple_agegroups` contributed data at multiple ages). Demographic exclusions were primarily implemented during recruitment; despite this, additional infants were tested and excluded based on preset criteria (see Exclusions below for percentages).  -->
<!-- In addition, `r excluded_labs` labs registered to participate but failed to collect data from at least 10 included infants, and so their data were not included. -->
<!-- Information about all included labs is given in Table 1.  -->

<!-- ```{r bin_stats} -->
<!-- age_stat <- d_participants %>% -->
<!--   group_by(lab, subid) %>% -->
<!--   summarise(age_days = age_days[1]) %>% -->
<!--   ungroup %>% -->
<!--   summarise(mean = mean(age_days),  -->
<!--             min = min(age_days),  -->
<!--             max = max(age_days)) -->

<!-- age_bins <- d_participants %>% -->
<!--   group_by(lab, age_group) %>% -->
<!--   summarise(n_babies = length(unique(subid))) %>% -->
<!--   group_by(age_group) %>% -->
<!--   summarise(n_babies = sum(n_babies),  -->
<!--             n_labs = length(unique(lab))) %>% -->
<!--   arrange(age_group) -->

<!-- nae_bins <- d_participants %>% -->
<!--   group_by(lab, subid, nae) %>% -->
<!--   summarise(n = length(unique(subid))) %>% -->
<!--   group_by(nae) %>% -->
<!--   summarise(n_babies = sum(n),  -->
<!--             n_labs = length(unique(lab))) -->
<!-- ``` -->


<!-- The mean age of infants included in the study was `r round(age_stat$mean, 2)` days (range: `r age_stat$min` -- `r age_stat$max`). There were `r age_bins$n_babies[age_bins$age_group == "3-6 mo"]` infants in the 3- to 6-month-old bin (`r age_bins$n_labs[age_bins$age_group == "3-6 mo"]` labs), `r age_bins$n_babies[age_bins$age_group == "6-9 mo"]` infants in the 6- to 9-month-old bin (`r age_bins$n_labs[age_bins$age_group == "6-9 mo"]` labs), `r age_bins$n_babies[age_bins$age_group == "9-12 mo"]` infants in the 9- to 12-month-old bin (`r age_bins$n_labs[age_bins$age_group == "9-12 mo"]` labs), and `r age_bins$n_babies[age_bins$age_group == "12-15 mo"]` infants in the 12- to 15-month-old bin (`r age_bins$n_labs[age_bins$age_group == "12-15 mo"]` labs). Many labs collected data in more than one bin. Of the total sample,  `r nae_bins$n_babies[nae_bins$nae]` infants (from `r nae_bins$n_labs[nae_bins$nae]` labs) were acquiring NAE, and `r nae_bins$n_babies[!nae_bins$nae]` infants (from `r nae_bins$n_labs[!nae_bins$nae]` labs) were acquiring a language other than NAE. As discussed above, a separate sample of bilingual children was tested in a parallel investigation, but these data are not reported in the current manuscript. -->

<!-- ```{r results="asis"} -->
<!-- langs <- read_csv(here("metadata","languages_per_lab_simple.csv")) -->

<!-- lab_stats <- d_participants %>% -->
<!--   mutate(method = case_when( -->
<!--     method == "singlescreen" ~ "central fixation", -->
<!--     method == "eyetracking" ~ "eye tracking", -->
<!--     method == "hpp" ~ "HPP", -->
<!--     TRUE ~ method))  %>%  -->
<!--   group_by(lab, subid) %>% -->
<!--   summarise(age_days = age_days[1],  -->
<!--             method = method[1]) %>% -->
<!--   group_by(lab) %>% -->
<!--   summarise(`Mean age (days)` = signif(mean(age_days, na.rm = TRUE), 3), -->
<!--             N = n(),  -->
<!--             Method = paste(unique(method[!is.na(method)]), collapse = ", ")) %>% -->
<!--   arrange(Method, lab) %>% -->
<!--   left_join(select(langs, lab, language_simple, country)) %>% -->
<!--   rename(Country = country,Language = language_simple) -->

<!-- papaja::apa_table(lab_stats, -->
<!--                   caption = "Statistics of the included labs. N refers to the number of infants included in the final analysis. English from the US and Canada are both treated as North American English.", -->
<!--                   format.args = list(digits = 0), -->
<!--                   longtable = TRUE,font_size="footnotesize", -->
<!--                   align=c("l","c","c","c","l","l"), -->
<!--                   col.names = c("lab","Mean age (days)","$N$", "Method","Language","Country")) -->
<!-- ``` -->


<!-- ## Materials -->

<!-- ### Visual stimuli -->

<!-- For labs using central fixation or eye tracking methods, a brightly colored static checkerboard was used as the fixation stimulus, and a small engaging video (an animation of colorful rings decreasing in size) as an attention-getter. For labs using HPP, we asked labs to use their typical visual stimulus, which varied considerably across laboratories. Some labs used flashing lights as the visual fixation stimulus (the original protocol that was developed in the 1980s), while others used a variety of other visual displays on video screens (e.g., a looming circle).  -->
<!-- <!-- Details regarding each laboratory’s implementation of a particular method are available at LINK. --> -->

<!-- ### Speech stimuli -->

<!-- The goal of our stimulus creation effort was to construct a set of recordings of naturalistic IDS and ADS gathered from a variety of mothers speaking to their infants. To do so, we gathered a set of recordings of mothers speaking to their infants and to experimenters, selected a subset of individual utterances from these (see below), and then constructed stimulus items from this subset. All other characteristics of the recordings besides register (IDS vs. ADS) were as balanced as possible across clips. Based on our intuitions and the data from the norming ratings described below, we consider these stimuli to be representative of naturally produced IDS and ADS across middle- and high-SES mothers in North America. Although future studies could attempt to vary particular aspects of the IDS systematically (e.g., age of the mother, age of the infant being spoken to, dialect), we did not do so here. Our stimulus elicitation method was designed to meet the competing considerations of laboratory control and naturalism. -->

<!-- Source recordings were collected in two laboratories, one in central Canada and one in the Northeastern United States. The recorded mothers had infants whose ages ranged from 122 – 250 days. The same recording procedures were followed in both laboratories. Recordings were collected in an infant-friendly greeting area/testing room using a simple lapel clip-on microphone connected to a smartphone (iPhone 5s or 6s), with the “Voice Record” or “Voice Record Pro” apps (Dayana Networks Ltd.) in the Canadian lab, and the “Voice Memos” app (Apple Inc.) in the US lab. The targets for conversation were objects in an opaque bag: five familiar objects (a ball, a shoe, a cup, a block, a train) and five unfamiliar objects (a sieve, a globe, a whisk, a flag, and a bag of yeast). To ensure that mothers used consistent labels, a small sticker was affixed to each object showing its name. Each object was taken out of the bag one at a time and the mother was asked to talk about the object, either to her baby (for the IDS samples) or to an experimenter (for the ADS samples) until she ran out of things to say; at this point the next object was taken out of the bag. Recording stopped when all the objects had been removed from the bag and had been talked about. Order of IDS and ADS recording was counterbalanced across participants. A total of 11 mothers were recorded in Canada and four in the United States. -->

<!-- There were a total of 179 unedited minutes of recording from Canada and 44 from the United States. A first-pass selection of low-noise IDS and ADS samples yielded 1281 utterances, for a total of 4479 s. From this first pass, 238 utterances were selected that were considered to be the best examples of IDS and ADS and met other basic stimulus selection criteria (e.g., did not contain laughter or the baby’s name).  -->

<!-- This library of 238 utterances was then normed on five variables: accent, affect, naturalness, noisiness, and IDS-ness. The goal of this norming was to gather intuitive judgments about each variable so as to identify utterances that were clearly anomalous in some respect and exclude them. In each case, a set of naïve, North American English-speaking adults recruited from Amazon Mechanical Turk (MTurk) listened to all 238 of the utterances and rated them on a 7-point Likert scale. Raters were assigned randomly to one of the five variables, with the number of participants assigned to a particular rating task ranging between eight and 18 due to variability in random assignment. Affect and IDS ratings were made using low-pass filtered recordings (a 120-Hz filter with standard rolloff was applied twice using the `sox` software package). These ratings were intended to give us a principled basis on which to exclude clips that were outliers on particular dimensions (such as having odd affect or background noise). In general, with the exception of IDS-ness, ratings were not highly variable across clips (the largest *SD* was .85, for noise ratings).  -->

<!-- Ratings from the tasks were then used to produce a set of utterances such that accent was rated similar to “standard English” (ratings < 3, with 1 being completely standard), naturalness was rated high (> 4, with 7 being completely natural), noisiness was rated low (< 4, with 1 being noiseless), and IDS and ADS clips were consistently distinguished (with IDS having ratings > 4 and ADS having ratings < 4, with 7 being clearly directed at a baby or child). This procedure resulted in 163 total utterances that met our inclusion criteria.  -->

<!-- Our next goal was to create eight IDS and eight ADS stimuli that were exactly 18 s in length, each containing utterances from the set we created. To do so, we assembled utterances from our filtered set. All clips were root mean square amplitude-normalized to 70 dB sound pressure level (SPL) before assembly, and then the final stimuli were amplitude-renormalized to 70 dB SPL. We assembled the final stimuli considering the following issues: -->

<!-- * *Identity*. Audio stimuli were constructed using clips from more than one mother. The number of different mothers included in a given stimulus was matched across IDS and ADS stimuli. In addition, multiple clips from the same mother were grouped together within a given stimulus in order to match the number of “mother transitions” across registers.   -->

<!-- * *Lexical items*. We matched the presence of object labels in the clips across IDS and ADS contexts. We also ensured an even distribution of the order in which each particular word was presented across stimuli and registers (ADS vs IDS).    -->

<!-- * *Questions*. IDS tends to include a much higher proportion of questions compared with ADS [@snow1977development; @soderstrom2008acoustical]. However, because the nature of the recording task may have served to inflate this difference, we preferentially selected declaratives over questions in the IDS sample. The final stimulus set contained 47% questions in the IDS samples and 3% questions in the ADS samples. We felt that retaining this naturally-occurring difference in IDS and ADS within our stimuli was more appropriate than precisely and artificially controlling for utterance-type across registers. -->

<!-- * *Duration of individual clips*. As expected, the utterances in IDS were much shorter than those in ADS, so it was not possible to match on duration or number of clips. Because there were more clips per stimulus in the IDS samples, there were also more utterances boundaries. This property is consistent with the literature on the natural characteristics of IDS [@martin2016utterances].   -->

<!-- * *Total duration*. We fixed all stimuli to have a total duration of 18 s by concatenating individual utterance files into single audio files that were > 18 s in length, trimming these down to 18 s and fading the audio in and out with 0.5 s half-cosine windows.  -->

<!-- Table 2 and Figure 1 provide additional details regarding the final stimulus set. Measurements were made using STRAIGHT [@kawahara2011technical], using default values for F0 extraction. For Figure 1, F0 values for voiced portions of the stimuli were collapsed into a series of logarithmically-spaced bins spanning the algorithm's F0 search range of 32-650 Hz.  -->

<!-- ```{r stimfreq, fig.cap="The distribution of F0 values for IDS and ADS is displayed as the proportion of voiced segments that fell in each F0 bin. Dashed lines show mean plus or minus one standard error across stimuli.", fig.align="center", out.width = "5in"} -->
<!-- include_graphics("plot_stim_freq.png") -->
<!-- ``` -->


<!-- ```{r stimtable, results="asis"} -->
<!-- stim_data <- tribble( -->
<!--   ~`Measurement`, ~`IDS mean`,  ~`IDS SD`, ~`ADS mean`, ~`ADS SD`, -->
<!--   "Number of mothers speaking per stimulus", 4.00, 0.00, 3.75, 0.46, -->
<!--   "Number of clips per stimulus", 6.88, 1.13, 4.50, 0.76, -->
<!--   "Number of objects mentioned per stimulus", 2.75, 0.71, 2.75, 0.71, -->
<!--   "Mean F0 (Hz) per stimulus",206.9, 19.5, 174.9, 13.2, -->
<!--   "10th percentile F0 (Hz) per stimulus", 131.4, 26.1, 139.0, 17.7,  -->
<!--   "90th percentile F0 (Hz) per stimulus", 340.0, 21.5, 232.0, 13.8, -->
<!--   "Mean number of utterances per stimulus", 7.75, 1.04, 6.63, 0.92, -->
<!--   "Mean duration (sec) of utterances", 1.58, 0.74, 2.12, 1.41, -->
<!--   "Mean inter-utterance interval (sec)", 0.75, 0.30, 0.59, 0.33 -->
<!-- ) -->

<!-- t1_caption <- "Characteristics of the IDS and ADS stimuli, with standard deviations computed across stimuli." -->

<!-- papaja::apa_table(stim_data, caption = t1_caption, -->
<!--                   col.names=c("Measurement", "IDS Mean","IDS $SD$","ADS Mean", "ADS $SD$")) -->
<!-- ``` -->

<!-- Table 3 provides a comparison of our stimuli to a sample of others that have been used previously in the IDS preference literature. Across studies, the only statistic that was reported reliably across papers was the mean pitch (F0) for IDS and ADS and even this one was only reported in about half the studies we sampled. Various measures of variability were reported in some studies (e.g., range within each sample, range across samples, standard deviation), but due to variation in the length and number of different samples used in each study, and a lack of systematicity in reporting, it was difficult to compare directly. Numerically, the average IDS/ADS pitch difference in our materials was less extreme than that found in previous studies.  -->

<!-- To confirm that our composite IDS and ADS stimuli were rated as natural and that the more limited pitch difference between registers still led to the stimuli being categorized differently, we conducted another norming study. Using the same basic paradigm as above, we collected a new sample of judgments from MTurk participants. Raters were randomly assigned to listen to all 16 stimuli and judge either whether they were directed at infants/children or adults ($N=22$) or else whether the stimuli sounded natural ($N=27$). All IDS clips were judged extremely likely to be directed at infants or children ($M = 6.74$, $SD = .09$, on a 1 – 7 rating scale), while all ADS clips were judged highly likely to be directed to adults ($M = 2.12$, $SD = .38$). Both were judged to be relatively natural, with the ADS, if anything, slightly more natural ($M = 5.18$, $SD = .19$) than the IDS ($M = 4.47$, $SD = .31$). In sum, because our stimuli were created from naturalistic productions from a wide range of mothers, they were less extreme in their intonation, but they were judged as natural and were easily identified as infant-directed. -->


<!-- ```{r comparisontable, results="asis"} -->
<!-- comp_data <- tribble( -->
<!--   ~`Study`, ~`Mean Ages (Months)`, ~`Context of Recording`, ~`Quantity of Stimuli`, ~`Mean IDS F0 (Hz)`, ~`Mean ADS F0 (Hz)`, ~`IDS-ADS (Hz)`, ~`IDS/ADS`,  -->
<!--   "Present Study", "3 – 15", "semi-structured, 4-8 month old child present", "8 full trial lengths' worth for each type", 206.90, 174.90, 32.0,1.18,  -->
<!--   "Cooper & Aslin (1990)", "0, 1", "read speech, no infant present", "4 sentences produced in each type", 315.88, 259.58, 56.3, 1.22, -->
<!--   "Newman & Hussain (2006)", "4.5, 9, 13", "read speech, no infant present", "4 passages produced in each type", 225.70, 189.65, 36.05, 1.19,  -->
<!--   "Thiessen et al. (2005)", "7", "nonsense strings of syllables, no infant present", "12 sentences in each style", 292, 230, 62, 1.27, -->
<!--   "Cooper et al. (1997)", "1, 4", "naturalistic speech to own infants", "20s of each style", 219.30, 184.30, 35, 1.19, -->
<!--   "Schachner & Hannon (2011)", "5", "elicited speech, with speaker looking at a picture", "1 min long videos, 2 in each style", 273.00, 224.70, 48.30, 1.21 -->
<!-- ) -->

<!-- t3_caption <- "Comparison of our study’s stimuli to those of previous studies on infant-directed speech preferences." -->

<!-- #papaja::apa_table(comp_data, caption = t3_caption, landscape = TRUE) -->

<!-- kable_input=kable(comp_data,caption = t3_caption,format = "latex",booktabs=TRUE, align = c("l","c","l","l",rep("c",4))) %>% -->
<!--   kable_styling(latex_options="scale_down") -->

<!-- landscape(kable_input, margin = NULL) -->
<!-- ``` -->
<!-- <!-- Table 3.  --> -->


<!-- ## Procedure -->

<!-- ### Basic Procedure  -->

<!-- Each lab used the testing paradigm(s) with which they were most familiar, among variants of three widely-used measurement methods: `r sum(labs$method == "hpp")` laboratories used the HPP, `r sum(labs$method == "singlescreen")` used the single-screen central visual-fixation preference procedure (CF), and `r sum(labs$method == "eyetracking")` used single-screen central visual fixation with fixations recorded by a corneal-reflection eye tracker (ET); four labs contributed data using two different methods. All procedural instructions to participant labs can be found at [https://osf.io/s3jca/]().  -->

<!-- To minimize researcher degrees of freedom, we asked participating labs to adhere to our instructions closely. Deviations from the basic protocol for each paradigm were necessary in some cases due to variation in the software and procedures used in each laboratory and were documented for future analysis.  -->

<!-- ### 1st vs. 2nd test session.  -->

<!-- In some laboratories, infants were sometimes tested in an unrelated experiment during their visit, either prior to or following the IDS preference experiment. Each lab noted whether infants completed the IDS preference experiment as their 1st (and possibly only) or 2nd test session.  -->

<!-- ### Onset of each trial -->

<!-- At the beginning of each trial, a centrally positioned visual stimulus (typically the study's standard attention getter, or a light in some HPP labs) was used to attract the infant’s attention. Upon fixation, this event was followed by a visual stimulus (a checkerboard for CF and ET, a light or a similar video for HPP). The stimulus appeared to the left or right of the infant in HPP setups and in the center in CF and ET setups. -->

<!-- ### Trials -->

<!-- At the beginning of the session, there were two warm-up trials that familiarized infants with the general procedure. The auditory stimulus for warm-up trials was an 18-second clip of piano music, and the visual stimulus was identical to the test trials. These trials familiarized infants to the general experimental setup and highlighted the contingency between looking at the visual display and the onset of the auditory stimulus. We did not analyze data from these trials. Training trials were then followed by up to 16 test trials presenting the IDS and ADS auditory stimuli. -->

<!-- ### Minimum looking time -->

<!-- There was no minimum required looking time during data collection (i.e., trials were never repeated). A minimum looking time of 2 s was used during analysis for inclusion of a trial. The 2-s minimum trial time was chosen after discussion across laboratories regarding typical standards of practice on minimum trial length, which varied considerably across laboratories. This criterion was selected to ensure that the infant had sufficient time to hear enough of the stimulus to discriminate IDS from ADS.  -->

<!-- ### Maximum looking time -->

<!-- On each test trial, infants could hear speech for a maximum of 18 s, corresponding to the duration of each sound file. For labs whose software could implement infant-controlled trial lengths, the trial ended if the infant looked away from the visual stimulus for two consecutive seconds. Otherwise, the trial continued until the stimulus ended. -->

<!-- ### Randomization -->

<!-- Four pseudo-random trial orders were created. Each order contained four blocks, with each block containing two IDS and two ADS trials in alternating order. Two blocks in each order began with IDS and the other two began with ADS. To facilitate analyses of preference scores by item, the same IDS and ADS stimuli were always paired with one another. -->

<!-- ### Volume -->

<!-- ```{r} -->
<!-- dB <- read_csv(here("metadata/labs_dB_levels.csv")) -->

<!-- dB <- dB %>%  -->
<!--   mutate(SNR = with_reference / without_reference) -->
<!-- ``` -->

<!-- Each lab was asked to use a stimulus volume level that was consistent with their general lab practices – this decision was not standardized across labs. Labs were instead instructed to measure and report their average dB SPL level with and without a white noise reference audio clip playing, though not all contributing labs reported these measurements ($N = `r sum(!is.na(dB$SNR))`$). From these values, we calculated a signal to noise ratio for each lab, $M = `r round(mean(dB$SNR, na.rm = TRUE), 2)`$, $SD = `r round(sd(dB$SNR, na.rm = TRUE), 2)`$, range: `r round(min(dB$SNR, na.rm = TRUE), 2)` -- `r round(max(dB$SNR, na.rm = TRUE), 2)`.  -->

<!-- ### Minimizing caregiver bias -->

<!-- We created a custom blend of instrumental music and a pastiche of stimulus materials triggered at random times and with random amplitude (available as part of the study materials). This masking stimulus was played to the caregiver over noise-attenuating headphones, to mask the IDS/ADS stimuli that the infant was hearing via external loudspeakers. Experimenters were instructed to play the masking music at a high (but comfortable and safe) volume. -->

<!-- ### Coding -->

<!-- Coding of looking times was conducted via the standard procedure in each lab. There were three methods of coding infant eye gaze: online coding by an experimenter via button press during the experimental session, offline coding of a video after the experimental session, or automatic coding collected by an eye tracker.  -->
<!-- <!-- XXX laboratories provided online data only, YYY laboratories provided offline data only, ZZZ laboratories analyzed gaze via eye tracking, and QQQ laboratories provided both online and offline coding.  --> -->
<!-- In the case that we received online and offline coding data, we used the offline coding.  -->

<!-- ### Minimizing experimenter bias -->

<!-- Experimenters making online coding decisions (in CF and HPP methods) were blind to the particular stimulus presented during testing trials, as they were either located in a different room from the infant, or were in the same room but were wearing noise-attenuating headphones and hearing the same masking stimuli as the infant's caregiver. Offline coding was conducted without direct access to the auditory stimuli. -->

<!-- ### Demographics -->

<!-- All labs were instructed to collect a set of basic participant demographic information: sex, date of birth, estimated proportion language exposure for the language(s) that they hear in their daily life, race/ethnicity (using categories appropriate for the cultural and geographic context), preterm/fullterm status, history of ear infections, known hearing or visual impairments, and known developmental concerns (e.g., developmental disorders). Parents were also asked to report information about themselves (gender, level of education, and native language/languages) and the child's siblings (sex/gender and date of birth). A standard recommended participant questionnaire was distributed to participating labs as part of the instructions, although labs were permitted to use their own forms as long as they gathered the necessary information. In addition, a subset of participating laboratories provided extensive additional information about infants and testing circumstances (not analyzed here), for use in planned followup projects. -->

<!-- <!-- FIXME - LAB LEVEL --> -->

<!-- ## General Lab Practices -->

<!-- ### Training of research assistants -->

<!-- Each lab was responsible for maintaining good experimenter training practices, and was expected to use the same rigor with the ManyBabies study as with any other study in their laboratory. Laboratories reported on which research assistant ran each infant using pseudonyms or numerical codes. Each laboratory completed a questionnaire regarding their training practices, the experience and academic status of each experimenter, and their basic participant greeting practices.  -->

<!-- ### Reporting of technology mishaps and infant/parent behavior -->

<!-- Laboratories were asked to note relevant concerns, anomalies and comments according to their standard lab practices and these were provided along with the looking time data and converted to a standardized form during the central analysis. Examples of relevant concerns included the infant crying during testing, parents intervening in a way that would affect their infant’s looking behavior (e.g., talking or pointing), or technical problems that prevented the normal presentation of experimental stimuli.  -->

<!-- ## Videos -->

<!-- All laboratories provided a "walk-through" video that detailed their basic processes including greeting, consent and data collection and showing the physical characteristics of their laboratory. (In our preregistration we stated that further procedural documentation would be available, but standardized reporting for procedural decision-making proved difficult to develop and deploy.)  In addition, we strongly encouraged laboratories to collect and share video recordings of their data collection according to what was permissible given their ethics approval and participant consent. If labs could not provide participant videos, they were asked to provide a video showing a run-through of their procedure and/or pictures and information regarding the study setup. A number of laboratories contributed these video recordings to Databrary, where they can be found by searching for "ManyBabies 1." -->

<!-- ## Exclusion Criteria -->

<!-- ```{r exclusions, child = "exclusions.Rmd"} -->
<!-- ``` -->

<!-- ## Post-Data Collection Methods Addendum -->

<!-- As the first experimental cross-laboratory infant study of this scale, there were a number of unanticipated issues that arose during data collection within individual labs and at the study level, which resulted in deviations from our registered protocol. All such cases were documented and decisions were made without consideration of their impact on the results. Fuller documentation can be found accompanying our shared data; here we summarize the nature and extent of these deviations. Note that some of these deviations were the result of typical within-laboratory protocol deviation (experimenter error, etc.) while others stemmed from the additional challenges inherent in harmonizing methodology and data format across such a large number of laboratories with different lab-internal protocols and standards.  -->

<!-- These protocol deviations include the following: -->

<!-- * Before labs had commenced data collection, we altered our attention-getter stimulus to be a precessing annulus accompanied by chimes (to address the concern that a laughing baby might be more associated with infant-directed speech); some labs used the old stimulus.  -->
<!-- * Variation in trial length beyond the assumed maximum of 18 s emerged due to deviations in lab’s protocols for a variety of reasons. In all cases, looking times on these trials were truncated to 18 s. -->
<!-- * A number of labs provided data from infants that were within the 3--15 month age range, but outside of the submitting lab’s pre-registered age bin. These infants were included in the analyses.  -->
<!-- * Many labs deviated from their pre-registered sample size due to constraints on testing resources. We included these labs provided they met the minimum inclusion criteria for the study as a whole. All such labs certified that they did not make decisions regarding sample size on a data-dependent basis. -->
<!-- * A number of laboratories marked participants as session-level errors for reasons other than equipment error, experimenter error or outside interference.  -->

<!-- This last point bears further discussion. Some labs marked participants as exclusions at the participant level for trial-level errors (e.g. infant fussy, parental interference), even though there was sufficient trial-level data available for analysis. Similarly, individual trials were sometimes marked as errors for reasons related to participant-level issues. All trial-level and participant-level errors were reviewed centrally by at least two coders using all available information in the spreadsheet to determine whether a trial-level or participant-level error was appropriate. Specific information about each trial or participant error coding that was changed during this process can be found by reviewing metadata within the data analysis codebase.  -->

<!-- In total, 313 participants from 50 labs previously marked as participant-level exclusions were retained for further processing and analysis. Participants originally coded as having session-level errors were recoded for the following reasons: when the participant-level exclusion was based solely on the existence of trial-level errors (190 infants), when exclusion was based on a different exclusion criterion (e.g., participants were out of the age range or were preterm) (93 infants), or if an issue identified by the lab at the participant level was deemed acceptable by the central analysis team (e.g., if a lab implemented a slightly different look-away criterion, see below) (30 infants). Note that many of the retained participants were subsequently excluded at other points in the analysis pipeline because, although they did not meet the criteria for session-level errors, they did meet the conditions for other exclusion criteria (e.g., participants did not contribute enough useable trials or were excluded based on language exposure).  -->

<!-- In addition to recoding session-level errors, we also corrected the coding of trial-level errors where appropriate. 778 total trial-level errors from 62 participants in 16 different labs were recoded. The majority of trials were corrected when labs coded a participant-level error (e.g. age exclusion) on the trial level (584 trials) or coded a trial-level error on the participant level (e.g., if labs marked a participant as a session-level error for fussiness on a specific trial, but did not code the affected trials as errors) (133 trials). Other trials were corrected when subsequent investigation of lab notes and discussion with lab members revealed that the original trial-level error code needed to be changed (61 trials). -->

<!-- In addition, a variety of errors were found (e.g., pilot participants not properly excluded but noted in the comments) and fixed within the spreadsheets. Video data were not reviewed centrally, although in some cases where a question arose, the laboratory reviewed their own video in-house in order to respond. The entire process has been carefully documented and can be accessed upon request, but because in some cases this included identifiable information about participants, it is not possible to share it publicly. -->

<!-- Other reported protocol deviations included: No preregistration form submitted (1 lab); trial look-away time set to 3 s for some participants (1 lab); lab temporarily moved location during data collection (1 lab); minor protocol technical changes after start of data collection (2 labs); alternated left-right presentation and tested skin conduction during procedure (1 lab); procedural differences related to high-chair usage (1 lab); attention-getter deviation (4 labs); use of a pinwheel rather than checkerboard as the main visual fixation stimulus in HPP (1 lab). -->

<!-- We also detected a large number of data submission errors (typographical or otherwise) as a result of the comprehensive checking process in analysis. These were resolved when necessary by contacting the original lab. In general, we were inclusive of data with minor protocol deviations, and erred on the side of excluding data, when necessary, at the trial rather than participant level. A few demographic variables required greater central scrutiny than originally anticipated. Most notably, there was considerable variability in the interpretation of preterm and bilingual designations (despite centrally-dictated standards). When necessary, we recoded lab data so as to conform to the original protocol definitions.  -->

<!-- There was an ambiguity in our lab-level exclusion criteria between whether labs would be included if they contributed 10 or more datapoints, or more than 10 datapoints. We chose the more liberal of these two criteria. -->

<!-- Finally, two labs submitted data after the deadline. In one case this was due to a communication error; in the other case, the lab continued data collection, resulting in 8 additional infants being tested. Both datasets are included in the final analysis here.  -->

<!-- # Results -->

<!-- ## Confirmatory Analyses -->

<!-- ### Data processing and analytic framework.  -->

<!-- All planned analyses were pre-registered in our initial registered report submission (available at [https://osf.io/vd789/]()). Our primary dependent variable of interest was looking time (LT). Looking time was defined as time spent fixating the screen (for central fixation and eye tracking methods, and some HPP set-ups) or light (HPP) during test trials; LT scores did not count any time spent looking away from the screen, even if looks away were below the threshold for terminating a trial. Since looking times are non-normally distributed, following @csibra2016statistical, we log-transformed all looking times prior to statistical analysis (we refer to this transformed variable as "log LT").  -->

<!-- We adopted two complementary analytic frameworks: meta-analysis and mixed-effects regression. In the meta-analytic framework, we conducted standard analyses within each lab and then estimated variability in the result of this analysis across labs. The meta-analytic approach has a number of advantages over the mixed-effects approach, including the use of simple within-lab analyses, the ability to estimate cross-lab variability directly, and the possibility of making direct comparisons with the standardized effect sizes that have been estimated in previous meta-analyses. However, the standard random-effects meta-analytic model is designed for a case where the raw data are unavailable and procedures and data-types are not standardized. In contrast, in our situation, procedures and data were standardized across labs and relevant moderators were recorded. The availability of trial-by-trial data across all labs allows us to use mixed-effects models, which account for the nesting and crossing of random effects (e.g., subjects nested within labs, items crossed across labs), and can provide more accurate estimates of the main effect and moderators. Both analyses were therefore included to allow for the most comprehensive understanding of the variance in the data. -->

<!-- Our meta-analyses were conducted as follows. The datasets provided by each lab were considered as separate “studies.” For each lab’s dataset, we first computed individual infants' IDS preference by 1) subtracting looking times to each IDS trial from its paired ADS trial (excluding trial pairs with missing data) and 2) computing a mean difference score (across trial pairs). Then we computed a group IDS preference for each lab and infant age group using $dz$, a version of Cohen’s standard *d* statistic, computed as the average of infants’ IDS preference scores divided by the standard deviation of those scores. We then used standard random effects meta-analysis fit using REML with the `metafor` package [@viechtbauer2010conducting].  -->

<!-- In our initial analysis plan, we did not anticipate that a large number of labs would collect data outside of their planned samples. For example, many labs contributed a sample of children within a specific age bin as well as several children that fell outside of that age bin, or a sample of children using one method and a handful of children with another. While we include these children in the mixed-effects analyses described below, we worried that the inclusion of many unplanned samples of just one or two infants in the meta-analytic models would excessively increase lab-level variance. Thus, for only the meta-analyses, we include only samples (e.g., age, language, or method groups) with ten or more infants.   -->

<!-- Our mixed effects models, fit to the entire dataset collected from the `r n_labs` labs, were specified as: -->

<!-- $$DV \sim IV_1 + IV_2 + \text{...} + (\text{...} | \text{subject}) + (\text{...} | \text{item}) + (\text{...} | \text{lab})$$ -->

<!-- The goal of this framework was to examine effects of the independent variables (notated IV) on the dependent variable (DV), while controlling for variation in both the DV (“random intercepts”) and the relationship of the IV to the DV (“random slopes”) based on relevant grouping units (subjects, items, and labs). The use of mixed-effects models also allowed us to move away from using difference scores as the dependent variable of interest. While difference scores simplify the process of calculating effect sizes for the meta-regression, their use requires that trials be paired, so some collected data (i.e., unpaired trials) cannot be analyzed. In the mixed effects framework, in contrast, looking time on individual trials is the dependent measure, ensuring that all trials can be included. -->

<!-- In our mixed-effects models, we planned a maximal random effects structure [@barr2013random], which entails specifying all random effects that are appropriate for the experimental design (e.g., IDS/ADS trial type can be nested within subjects – since each infant heard stimuli in both conditions -– but cannot be nested within items since each item is unique to its trial type). In cases of mixed-effects models that failed to converge, we pursued an iterative pruning strategy. We began by removing random slopes nested within items (as that grouping was of least theoretical interest) and next removing random slopes nested within subjects and then labs. We then removed random intercepts from groupings in the same order, retaining effects of trial type until last since these were of greatest theoretical interest. We fit all models using the `lme4` package [@bates2014fitting] and computed $p$ values using the `lmerTest` package [@R-lmerTest]. -->

<!-- ### IDS preference -->


<!-- ```{r} -->
<!-- d <- read_csv(here("processed_data/03_data_trial_main.csv"),  -->
<!--               na = c("NA", "N/A")) %>% -->
<!--   mutate(method = case_when( -->
<!--     method == "singlescreen" ~ "Central fixation", -->
<!--     method == "eyetracking" ~ "Eye tracking", -->
<!--     method == "hpp" ~ "HPP", -->
<!--     TRUE ~ method))  -->
<!-- diffs <- read_csv(here("processed_data/03_data_diff_main.csv"), -->
<!--                   na = c("NA", "N/A")) %>% -->
<!--   mutate(method = case_when( -->
<!--     method == "singlescreen" ~ "Central fixation", -->
<!--     method == "eyetracking" ~ "Eye tracking", -->
<!--     method == "hpp" ~ "HPP", -->
<!--     TRUE ~ method))  -->

<!-- ordered_ages <- c("3-6 mo", "6-9 mo", "9-12 mo", "12-15 mo") -->
<!-- d$age_group <- fct_relevel(d$age_group, ordered_ages) -->
<!-- diffs$age_group <- fct_relevel(diffs$age_group, ordered_ages) -->

<!-- source(here("helper/ma_helper.R")) -->

<!-- ages <- d %>% -->
<!--   group_by(lab, age_group, method, nae, subid) %>% -->
<!--   summarise(age_mo = mean(age_mo)) %>% -->
<!--   summarise(age_mo = mean(age_mo)) -->

<!-- ds_zt <- diffs %>% -->
<!--   group_by(lab, age_group, method, nae, subid) %>% -->
<!--   summarise(d = mean(diff, na.rm = TRUE)) %>% -->
<!--   group_by(lab, age_group, method, nae) %>% -->
<!--   summarise(d_z = mean(d, na.rm = TRUE) / sd(d, na.rm = TRUE),  -->
<!--             n = length(unique(subid)),  -->
<!--             d_z_var = d_var_calc(n, d_z)) %>% -->
<!--   filter(n >= 10) %>% -->
<!--   left_join(ages) %>% -->
<!--   filter(!is.na(d_z)) # CHECK THIS  -->

<!-- intercept_mod <- metafor::rma(d_z ~ 1,  -->
<!--                               vi = d_z_var, slab = lab, data = ds_zt,  -->
<!--                               method = "REML")  -->
<!-- ``` -->

<!-- ```{r infant_global_pref} -->
<!-- pref <- diffs %>% -->
<!--   group_by(lab, subid) %>% -->
<!--   summarise(pref = sum(diff, na.rm = TRUE) > 0) %>% -->
<!--   ungroup() %>% -->
<!--   summarise(pct = mean(pref)*100,  -->
<!--             pref = sum(pref),  -->
<!--             n = n()) -->

<!-- ``` -->


<!-- What was the overall magnitude of the IDS preference we observed? This question is answered within the cross-lab meta-analysis by fitting the main effect model specified by $dz \sim 1$ to the `r nrow(ds_zt)` separate group means and variances (after aggregating by lab and age group). The mean effect size estimate was `r round(intercept_mod$b[1], 2)` (CI = [`r round(intercept_mod$ci.lb[1], 2)` - `r round(intercept_mod$ci.ub[1], 2)`], $z = `r round(intercept_mod$z[1], 2)`$, $p `r papaja::printp(intercept_mod$pval)`$). A forest plot for this meta-analysis is shown in Figure 2. Further, `r pref$pref`/`r pref$n` infants (`r round(pref$pct, 2)`%) showed a numerical preference for IDS.  -->


<!-- ```{r fig.cap="Forest plot. Standardized effect sizes are shown for each lab, with error bars showing 95% confidence intervals. Labs are grouped by method. Points are scaled by inverse variance and colored by experimental method. In each panel, the diamond and associated interval represents the meta-analytic estimate from the method-moderated model and its 95% confidence interval. The bottom panel shows the global meta-analytic estimate from the unmoderated model.", fig.height = 5.5, fig.pos="p!"} -->

<!-- ds_zt$age_mo_centered <- scale(ds_zt$age_mo, scale = FALSE)[,1] -->

<!-- age_mod <- metafor::rma(d_z ~ age_mo_centered,  -->
<!--                         vi = d_z_var, slab = lab, data = ds_zt, method = "REML")  -->

<!-- lang_mod <- metafor::rma(d_z ~ nae, vi = d_z_var,  -->
<!--                          slab = lab, data = ds_zt, method = "REML")  -->

<!-- method_mod <- metafor::rma(d_z ~ method, vi = d_z_var,  -->
<!--                            slab = lab, data = ds_zt, method = "REML")  -->

<!-- # get fitted estimates for all labs -->
<!-- f <- fitted(intercept_mod) -->
<!-- p <- predict(intercept_mod) -->

<!-- alpha <- .05 -->

<!-- forest_data <- data.frame(effects = as.numeric(intercept_mod$yi.f), -->
<!--                           variances = intercept_mod$vi.f) %>% -->
<!--   mutate(effects.cil = effects - -->
<!--            qnorm(alpha / 2, lower.tail = FALSE) * sqrt(variances), -->
<!--          effects.cih = effects + -->
<!--            qnorm(alpha / 2, lower.tail = FALSE) * sqrt(variances), -->
<!--          estimate = as.numeric(f), -->
<!--          lab = factor(names(f)), -->
<!--          estimate.cil = p$ci.lb, -->
<!--          estimate.cih = p$ci.ub, -->
<!--          inverse_vars = 1/variances, -->
<!--          identity = 1,  -->
<!--          lab = str_replace(lab, "\\.[1-9]",""),  -->
<!--          index = 1:n()) %>% -->
<!--   bind_cols(ungroup(ds_zt) %>% select(lab, method)) -->

<!-- # predict MA means for methods -->
<!-- mf <- fitted(method_mod) -->
<!-- mp <- predict(method_mod, -->
<!--               newmods = t(cbind(c(0,0), # intercept - central fixation -->
<!--                                 c(1,0), # eye-tracking -->
<!--                                 c(0,1))), # HPP -->
<!--               intercept = TRUE) -->


<!-- # Add meta-analytic estimate -->
<!-- forest_data <- bind_rows(forest_data, -->
<!--                          data_frame(lab = "Meta-analytic estimate", -->
<!--                                     method = "", -->
<!--                                     effects = summary(intercept_mod)$b[1], -->
<!--                                     effects.cil = summary(intercept_mod)$ci.lb, -->
<!--                                     effects.cih = summary(intercept_mod)$ci.ub), -->
<!--                          data_frame(lab = "Meta-analytic estimate", -->
<!--                                     method = c("Central fixation","Eye tracking", -->
<!--                                                "HPP"), -->
<!--                                     effects = mp$pred, -->
<!--                                     effects.cil = mp$ci.lb, -->
<!--                                     effects.cih = mp$ci.ub)) %>% -->
<!--   mutate(method = fct_rev(fct_relevel(method, "")), -->
<!--          lab = fct_relevel(lab, "Meta-analytic estimate"))  -->

<!-- # plot -->
<!-- ggplot(forest_data, aes(x = lab, y = effects)) +  -->
<!--   geom_hline(yintercept = 0, linetype = "dashed", color = "grey") + -->
<!--   geom_linerange(aes(ymin = effects - sqrt(variances)*1.96, -->
<!--                      ymax = effects + sqrt(variances)*1.96,  -->
<!--                      group = index), -->
<!--                  alpha = .5, -->
<!--                  position = position_dodge(width = .5)) + -->
<!--   geom_point(data = filter(forest_data, lab != "Meta-analytic estimate"), -->
<!--              aes(y = effects, size = inverse_vars, col = method,  -->
<!--                  group = index),  -->
<!--              alpha = .5,  -->
<!--              position = position_dodge(width = .5)) + -->
<!--   geom_point(data = filter(forest_data, lab == "Meta-analytic estimate"), -->
<!--              pch = 5) + -->
<!--   geom_linerange(data = filter(forest_data, lab == "Meta-analytic estimate"), -->
<!--                  aes(ymin = effects.cil, ymax = effects.cih),  -->
<!--                  alpha = .5) + -->
<!--   facet_grid(method ~ ., scales = "free", space = "free") + -->
<!--   coord_flip() + -->
<!--   scale_size_continuous(guide = FALSE) + -->
<!--   scale_colour_ptol(guide = FALSE) + -->
<!--   xlab("Lab") + -->
<!--   ylab("Effect Size") + -->
<!--   theme(axis.text.y = element_text(size = 6)) -->
<!-- ``` -->


<!-- ### Independent relationship of IDS preference to moderating variables -->

<!-- We next fit a set of moderated meta-analytic models. We began by examining the relationship of IDS preferences to age, using the average age in months for each lab’s contributed sample as the moderator value. Labs that contributed samples from two age bins had values added separately for each age (because of the small number of these, we did not model this dependency between labs). For ease of interpretation, we centered age in this analysis. The age-moderated model, $dz \sim 1 + \text{age}$, yielded an estimated main effect of  `r round(age_mod$b[1], 2)` (CI = [`r round(age_mod$ci.lb[1], 2)` - `r round(age_mod$ci.ub[1], 2)`], $z = `r round(age_mod$z[1], 2)`$, $p `r papaja::printp(age_mod$pval[1])`$) and an age effect of `r round(age_mod$b[2], 2)` (CI = [`r round(age_mod$ci.lb[2], 2)` - `r round(age_mod$ci.ub[2], 2)`], $z = `r round(age_mod$z[2], 2)`$, $p `r papaja::printp(age_mod$pval[2])`$). This positive age coefficient indicated that the measured IDS preference was on average larger for older children. Age trends are plotted in Figure 3. -->

<!-- ```{r fig3, fig.cap = "Lab effect size estimates plotted by age and method. Subplots show language groups. Standardized effect sizes are shown for each lab, with error bars showing 95% confidence intervals. Points are scaled by number of participants and colored by experimental method; they are slightly transparent to avoid overplotting."} -->
<!-- ds_zt$english <- factor(ds_zt$nae, levels = c(TRUE, FALSE),  -->
<!--                         labels = c("North American English", "Non-North American English"))  -->

<!-- ggplot(ds_zt,  -->
<!--        aes(x = age_mo, y = d_z)) +  -->
<!--   geom_point(aes(size = n, col = method), alpha = .3) +  -->
<!--   geom_linerange(aes(ymin = d_z - 1.96 * sqrt(d_z_var),  -->
<!--                      ymax = d_z + 1.96 * sqrt(d_z_var), col = method)) +  -->
<!--   geom_hline(yintercept = 0, linetype = "dashed", color = "grey") + -->
<!--   geom_smooth(method = "lm") +  -->
<!--   facet_grid(~english) +  -->
<!--   scale_colour_ptol(name = "Method") + -->
<!--   scale_size_continuous(guide = FALSE) + -->
<!--   xlab("Mean Age (Months)") + -->
<!--   ylab("Effect Size") +  -->
<!--   theme(legend.position = "bottom") -->
<!-- ``` -->

<!-- We next investigated effects of experimental method, with method dummy-coded using single-screen central fixation as the reference level. The method-moderated model ($dz \sim 1 + \text{method})$ yielded a reference-level intercept of `r round(method_mod$b[1], 2)` (CI = [`r round(method_mod$ci.lb[1], 2)` - `r round(method_mod$ci.ub[1], 2)`], $z = `r round(method_mod$z[1], 2)`$, $p `r papaja::printp(method_mod$pval[1])`$), reflecting the mean effect size for single-screen presentation. The HPP yielded an additional effect of `r round(method_mod$b[3], 2)` (CI = [`r round(method_mod$ci.lb[3], 2)` - `r round(method_mod$ci.ub[3], 2)`], $z = `r round(method_mod$z[3], 2)`$, $p = `r papaja::printp(method_mod$pval[3])`$), indicating a substantial gain in measured IDS preference for those labs using HPP as compared with single-screen central fixation. In contrast, eye-tracking yielded an effect of `r round(method_mod$b[2], 2)` (CI = [`r round(method_mod$ci.lb[2], 2)` - `r round(method_mod$ci.ub[2], 2)`], $z = `r round(method_mod$z[2], 2)`$, $p = `r papaja::printp(method_mod$pval[2])`$), indicating a slight, non-significant decrease in measured effect size for eye-tracking relative to single-screen central fixation.  -->

<!-- The language-moderated model ($dz \sim 1 + \text{language}$) was fit with language group coded as a categorical variable indicating whether infants were tested in a lab in which NAE was the standard language (e.g., in the United States or Canada). The reference level effect (i.e., not NAE) was `r round(lang_mod$b[1], 2)` (CI = [`r round(lang_mod$ci.lb[1], 2)` - `r round(lang_mod$ci.ub[1], 2)`], $z = `r round(lang_mod$z[1], 2)`$, $p `r papaja::printp(lang_mod$pval[1])`$), while for infants in North American labs, the effect was increased by  `r round(lang_mod$b[2], 2)` (CI = [`r round(lang_mod$ci.lb[2], 2)` - `r round(lang_mod$ci.ub[2], 2)`], $z = `r round(lang_mod$z[2], 2)`$, $p = `r papaja::printp(lang_mod$pval[2])`$). Thus, measured IDS preferences were higher in those infants for whom the stimuli were native-language congruent. -->


<!-- ### Joint relationship of IDS preference to moderating variables -->

<!-- Because infant age, language, and method were confounded across labs (labs with particular methods also chose specific sample age ranges, and these choices were not independent), we next turn to the mixed- effects modeling framework to estimate subject-level age effects and lab-level method effects. To help visualize the spread of subject-level effects, Figure 4 shows IDS preferences for individual participants. -->

<!-- Our main model was: -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \text{log lt} \sim & \text{trial type} * \text{method} + \text{trial type} * \text{trial num} + \text{age} * \text{trial num} + \\ -->
<!-- & \text{trial type} * \text{age} * \text{language} + \\ -->
<!-- & (\text{trial type} * \text{trial num} \mid \text{subid}) + \\ -->
<!-- & (\text{trial type} * \text{age} \mid \text{lab}) + \\ -->
<!-- & (\text{method} + \text{age} * \text{language} \mid \text{item}) -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- Trial type, language, and method were dummy-coded (with ADS trials, non-NAE, and single-screen method) as the reference level; thus, coefficients are interpretable such that e.g., positive effects of trial type indicate longer looking to IDS. To increase the interpretability of coefficients, age (in months) was centered and trial number was coded with trial 1 as the reference level. -->

<!-- We specified this model to minimize higher-order interactions but preserve theoretically-important interactions. We included main effects of trial type, method, language, age, and trial number, capturing the basic effects of each on looking time (e.g., longer looking times for IDS, shorter looking times on later trials). In addition, we included two-way interactions of trial type with method (modeling the possibility that some methods show larger IDS preferences) and trial type with trial number (modeling the possibility of faster habituation to ADS) as well as age and trial number (modeling faster habituation for older children). We also included two- and three-way interactions of age, trial type, and language (modeling possible developmental changes in IDS preference across age and language group). Both developmental effects and trial effects are treated linearly in this model; although both likely have non-linear effects, adding quadratic or other effects would have substantially increased model complexity. After pruning random effects for non-convergence,\footnote{Pruning was done using models fitted with `lme4` version 1.1-21.} our final model specification was: -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \text{log lt} \sim & \text{trial type} * \text{method} + \text{trial type} * \text{trial num} + \text{age} * \text{trial num} + \\ -->
<!-- & \text{trial type} * \text{age} * \text{language} + \\ -->
<!-- & (1 \mid \text{subid}) + \\ -->
<!-- & (1 \mid \text{lab}) + \\ -->
<!-- & (1 \mid \text{item}). -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- \noindent Table 4 shows coefficient estimates from this model.  -->

<!-- ```{r coef_table, results="asis"} -->
<!-- library(lmerTest) -->
<!-- d_lmer <- d %>% -->
<!--   filter(trial_type != "train") %>% -->
<!--   mutate(log_lt = log(looking_time), -->
<!--          age_mo = scale(age_mo, scale = FALSE), -->
<!--          trial_num = trial_num,  -->
<!--          item = paste0(stimulus_num, trial_type)) %>% -->
<!--   filter(!is.na(log_lt), !is.infinite(log_lt)) -->

<!-- mod_lmer <- lmer(log_lt ~ trial_type * method + -->
<!--                    trial_type * trial_num + -->
<!--                    age_mo * trial_num + -->
<!--                    trial_type * age_mo * nae + -->
<!--                    (1 | subid_unique) + -->
<!--                    (1 | item) +  -->
<!--                    (1 | lab),  -->
<!--                  data = d_lmer) -->

<!-- coefs <- summary(mod_lmer)$coef %>% -->
<!--   as_tibble %>% -->
<!--   mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"),  -->
<!--             function (x) signif(x, digits = 3)) %>% -->
<!--   rename(SE = `Std. Error`,  -->
<!--          t = `t value`, -->
<!--          p = `Pr(>|t|)`) %>% -->
<!--   select(-df) -->

<!-- rownames(coefs) <- c("Intercept", "IDS", "Eye-tracking", "HPP",  -->
<!--                      "Trial #", "Age", "NAE", "IDS * Eye-tracking",  -->
<!--                      "IDS * HPP",  -->
<!--                      "IDS * Trial #", "Trial # * Age", "IDS * Age", "IDS * NAE",  -->
<!--                      "Age * NAE", "IDS * Age * NAE") -->

<!-- papaja::apa_table(coefs,  -->
<!--                   caption = "Coefficient estimates from a linear mixed effects model predicting log looking time.",  -->
<!--                   format.args = list(digits = 3), -->
<!--                   col.names =c("","Estimate","$SE$","$t$","$p$"), -->
<!--                   align=c("l","l","c","c","c")) -->
<!-- ``` -->

<!-- Overall, the fitted coefficients of the mixed effects model were consistent with the results of the individual meta-analyses. Within the structure of the mixed effects model, IDS preferences are shown by positive coefficients on the IDS predictor (reflecting greater looking times to IDS stimuli). The fitted model shows a significant positive effect of IDS stimuli, consistent with a global IDS preference. Consistent with the age- and language-moderated meta-analyses, there were significant and positive two-way interations of IDS with age and with NAE, suggesting greater IDS preferences for older children and for children in NAE contexts. Further, there was a positive interaction with the HPP method, consistent with the method-moderated model. There was not a significant three-way interaction of IDS, age, and NAE, however, suggesting that there was not a reliable differential change in IDS preference for older children in NAE contexts over and above that expected based on each of these factors alone.  -->

<!-- In addition to these results, a number of other factors were significant predictors of looking time. Looking time decreased across trials, and did so especially for older children, generally confirming that all infants habituated to our experimental stimuli and older infants did so more quickly. Further, eye-tracking led to lower looking times overall across stimulus classes. -->

<!-- ```{r lookingtimes, fig.cap = "Simple linear trends for IDS preference by age and language group, plotted (A) with individual participants' preferences and (B) without individual participants' preferences to show trends more effectively."} -->

<!-- mss_diffs <- diffs %>% -->
<!--   group_by(lab, method, nae, subid) %>% -->
<!--   summarise(n = sum(!is.na(diff)), -->
<!--             age_mo = mean(age_mo),  -->
<!--             diff = mean(diff, na.rm=TRUE)) -->

<!-- mss_diffs_plot_b <- ggplot(mss_diffs, aes(x = age_mo, y = diff, col = method, lty = nae)) +  -->
<!--   geom_smooth(method = "lm", se=FALSE) +  -->
<!--   geom_hline(yintercept = 0, lty = 2) +  -->
<!--   ylab("IDS preference (s)") +  -->
<!--   scale_color_ptol(name = "Method") +  -->
<!--   scale_linetype(name = "North American English") +  -->
<!--   xlab("Age (Months)") + -->
<!--   theme(legend.title=element_text(size=10),legend.text=element_text(size=8)) -->

<!-- mss_diffs_plot_a <-mss_diffs_plot_b + -->
<!--   geom_point(data = filter(mss_diffs, n == 8), alpha = .1)  -->

<!-- ggarrange(mss_diffs_plot_a, mss_diffs_plot_b, labels = c("A", "B"), ncol = 2, nrow = 1, legend = "bottom", common.legend = TRUE) -->

<!-- ``` -->



<!-- ### Effects of second-session testing on IDS preference -->

<!-- ```{r secondsession} -->
<!-- d_second <- read_csv(here("processed_data/03_data_trial_2ndsess.csv"),  -->
<!--                      na = c("NA", "N/A"))  -->
<!-- diff_second <- read_csv(here("processed_data/03_data_diff_2ndsess.csv"),  -->
<!--                         na = c("NA", "N/A"))  -->

<!-- n_ss <- d_second %>% -->
<!--   filter(second_session) %>% -->
<!--   group_by(lab, subid) %>% -->
<!--   count %>% -->
<!--   group_by(lab) %>% -->
<!--   count -->


<!-- pref_second <- diff_second %>% -->
<!--   filter(second_session) %>% -->
<!--   group_by(lab, subid) %>% -->
<!--   summarise(pref = sum(diff, na.rm = TRUE) > 0) %>% -->
<!--   ungroup() %>% -->
<!--   summarise(pct = mean(pref)*100,  -->
<!--             pref = sum(pref),  -->
<!--             ci.l = binom::binom.bayes(x = sum(pref), n = n())$lower, -->
<!--             ci.u = binom::binom.bayes(x = sum(pref), n = n())$upper, -->
<!--             n = n()) -->

<!-- ``` -->

<!-- We preregistered an analysis of whether second-session infants showed a different pattern of infant-directed speech preference. Only `r nrow(n_ss)` labs contributed second-session infants, however, with a total of only `r sum(n_ss$n)` infants represented. Thus, we did not fit the full, pre-registered mixed-effects model for this variable as we did not have enough variability on the important covariates to estimate this variable.  As an exploratory analysis, we note that `r pref_second$pref`/`r pref_second$n` second-session infants (`r round(pref_second$pct, 1)`% [`r round(pref_second$ci.l, 4)*100` - `r round(pref_second$ci.u, 4)*100`]) showed a numerical preference for IDS. This number was numerically different  but not distinguishable statistically from the `r round(pref$pct, 2)`% of IDS preferences in the first-session infants, likely due to the small sample of second-session infants. -->



<!-- <!-- Because XYZ labs contributed both first- and second-session infants (with XYZ contributing only first and XYZ only second), we followed the mixed-effects approach described above. We fit the model specified above with the addition of a second-session main effect and trial type by second-session interaction (and with a second-session random slope and intercept nested within labs). The main effect of second session was $\beta$=XYZ (SE=XYZ, p=XYZ) and the interaction with trial type was $\beta$=XYZ (SE=XYZ, p=XYZ). --> -->

<!-- ### Sex and IDS preference -->


<!-- ```{r coef_table_sex, results="asis"} -->
<!-- sex_mod_lmer <- lmer(log_lt ~ trial_type * method + -->
<!--                        trial_type * trial_num + -->
<!--                        age_mo * trial_num + -->
<!--                        trial_type * age_mo * nae + -->
<!--                        gender * trial_type + -->
<!--                        (1 | subid_unique) + -->
<!--                        # (1 | item) +  -->
<!--                        (1 | lab),  -->
<!--                      data = filter(d_lmer,  -->
<!--                                    gender %in% c("M","F"))) -->

<!-- sex_coefs <- summary(sex_mod_lmer)$coef -->
<!-- ``` -->

<!-- <!-- and intercept  NOTE WE PREREGISTERED AN INTERCEPT AND SLOPE BUT THIS DOESN"T MAKE SENSE--> -->

<!-- In order to investigate effects of biological sex on IDS preference, we fit the model specified above with the addition of a sex main effect and trial type by sex interaction.\footnote{Because this model did not converge, following our protocol, we pruned random effects of item.} Female was coded as the reference level, so effects are stated in terms of changes for male infants. The main effect of sex $\beta = `r signif(sex_coefs["genderM","Estimate"], 2)`$ ($SE=`r signif(sex_coefs["genderM","Std. Error"], 2)`$, $p=`r signif(sex_coefs["genderM","Pr(>|t|)"], 2)`$) and the interaction with trial type was $\beta=`r signif(sex_coefs["trial_typeIDS:genderM","Estimate"], 3)`$ ($SE=`r signif(sex_coefs["trial_typeIDS:genderM","Std. Error"], 2)`$, $p=`r signif(sex_coefs["trial_typeIDS:genderM","Pr(>|t|)"], 2)`$). These predictors were small and nonsignificant, suggesting that sex was not a strong determinant of measured IDS preferences in our data.  -->

<!-- ### Moderator effects on missing data -->

<!-- One further question regarding our data was whether particular moderator variables affected not just the amount of looking time we recorded, but whether children looked at all during a trial. To test for effects of moderators on the presence of missing data, we constructed a categorical variable (missing), which was true if a trial had no included looking time (e.g., no looking recorded, a look under 2 s, or no looking because the infant had already terminated the experiment) and false otherwise. We fit a logistic version mixed-effects model with all two-way interactions between method, age, and trial number, using the specification: -->

<!-- <!-- NOTE: WE HAD A RANDOM EFFECT OF LANGUAGE BY ITEM BUT NO MAIN EFFECT IN THE PREREG- MAKES NO SENSE... DISREGARDED THIS --> -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \text{missing} \sim & \text{method} * \text{age} + \text{method} * \text{trial num} + \text{age} * \text{trial num} + \\ -->
<!-- & (1 \mid \text{subid}) + \\ -->
<!-- & (\text{trial num} * \text{age} \mid \text{lab}) + \\ -->
<!-- & (\text{method} + \text{age} \mid \text{item}). -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- \noindent After pruning for non-convergence, our final model specification was: -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \text{missing} \sim & \text{method} * \text{age} + \text{method} * \text{trial num} + \text{age} * \text{trial num} + \\ -->
<!-- & (1 \mid \text{lab}). -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- \noindent Table 5 shows coefficient estimates from this model. To aid convergence, we centered and scaled age and trial number, and set single screen presentation as the reference level. Positive coefficients indicate a higher probability of missing data. Older children and later trials had greater amounts of missing data, consistent with the idea that all children habituated to the stimuli, but that older children habituated faster. There was also a significant negative interaction of age and eye-tracking, suggesting that data loss for eye-tracking was substantially greater in younger children and lower in older children (we return to this issue in the general discussion). Other coefficients were relatively small and nonsignificant.  -->

<!-- ```{r coef_table_missing, results="asis"} -->
<!-- d_lmer_missing <- d %>% -->
<!--   mutate(age_mo = scale(age_mo, scale = TRUE), -->
<!--          trial_num = scale(trial_num, scale = TRUE),  -->
<!--          item = paste0(stimulus_num, trial_type),  -->
<!--          method = relevel(factor(method), ref = 1)) -->

<!-- missing_mod <- glmer(missing ~  method * age_mo +  -->
<!--                        method * trial_num +  -->
<!--                        age_mo * trial_num +  -->
<!--                        (1 | lab), -->
<!--                      family = "binomial", -->
<!--                      data = d_lmer_missing) -->

<!-- missing_coefs <- summary(missing_mod)$coef %>% -->
<!--   as.data.frame %>% -->
<!--   mutate_at(c("Estimate","Std. Error", "z value", "Pr(>|z|)"),  -->
<!--             function (x) signif(x, digits = 3)) %>%  -->
<!--   rename(SE = `Std. Error`,  -->
<!--          z = `z value`, -->
<!--          p = `Pr(>|z|)`) -->

<!-- rownames(missing_coefs) <- c("Intercept", "Eye-tracking", "HPP",  -->
<!--                              "Age", "Trial #", -->
<!--                              "Eye-tracking * Age",  -->
<!--                              "HPP * Age",  -->
<!--                              "Eye-tracking * Trial #",  -->
<!--                              "HPP * Trial #",  -->
<!--                              "Trial # * Age") -->

<!-- papaja::apa_table(missing_coefs, format.args = list(digits = 3), -->
<!--                   caption = "Coefficient estimates from a linear mixed effects model predicting whether an observation was missing.", -->
<!--                   col.names =c("","Estimate","$SE$","$z$","$p$"), -->
<!--                   align=c("l","l","c","c","c")) -->
<!-- ``` -->

<!-- ## Exploratory Analyses -->

<!-- ### Meta-analytic heterogeneity -->

<!-- One question of interest was whether we observed any meta-analytic heterogeneity in the data. When a meta-analysis shows heterogeneity, that finding indicates the presence of unexplained variance in effect size over and above that due to sampling variation; the $\tau^2$ provides an estimate of the total heterogeneity in our models. We further assess heterogeneity using the $I^2$ statistic [@higgins2003measuring], which quantifies the proportion of total variation in estimates that is due to heterogeneity. We also report the results of a standard hypothesis test for heterogeneity, the Cochran $Q$ test; when this test is statistically significant, that indicates that the null hypothesis of homogeneity of variance can be rejected [@huedo2016assessing]. -->

<!-- In our primary, intercept-only meta-analytic model, $\tau^2 = `r round(intercept_mod$tau2,digits = 2)`\%$, $I^2 = `r round(intercept_mod$I2,digits = 2)`\%$, and $Q(`r intercept_mod$k-intercept_mod$p`) = `r round(intercept_mod$QE,digits = 2)`$, $p = `r round(intercept_mod$QEp,digits = 2)`$. In the language-moderated model, $\tau^2 = `r round(lang_mod$tau2,digits = 2)`\%$, $I^2 = `r round(lang_mod$I2,digits = 2)`\%$, and $Q(`r lang_mod$k-lang_mod$p`) = `r round(lang_mod$QE,digits = 2)`$, $p = `r round(lang_mod$QEp,digits = 2)`$.  In the age-moderated model, $\tau^2 = `r round(age_mod$tau2,digits = 2)`\%$, $I^2 = `r round(age_mod$I2,digits = 2)`\%$, and $Q(`r age_mod$k-age_mod$p`) = `r round(age_mod$QE,digits = 2)`$, $p = `r round(age_mod$QEp,digits = 2)`$. Finally, in the method-moderated model, $\tau^2 = `r round(method_mod$tau2,digits = 2)`\%$, $I^2 = `r round(method_mod$I2,digits = 2)`\%$, and $Q(`r method_mod$k-method_mod$p`) = `r round(method_mod$QE,digits = 2)`$, $p = `r round(method_mod$QEp,digits = 2)`$. In none of these could we reject the null hypothesis of no heterogeneity beyond sampling variation, and in no case was the magnitude of observed heterogeneity large. Although there were reliable moderators (see meta-analytic results above), these moderators were quite small in magnitude relative to the sampling variation in individual lab effect size estimates (because of the small median sample size within each lab). -->

<!-- ### Exclusion criteria -->


<!-- ```{r,warnings=FALSE,echo=FALSE,message=FALSE} -->
<!-- ds_excl <- diffs %>%  -->
<!--   group_by(lab, age_group, method, nae, subid) %>% -->
<!--   summarise(excl2 = mean(diff, na.rm = TRUE), -->
<!--             excl4 = ifelse(sum(!is.na(diff)) > 2, -->
<!--                            mean(diff, na.rm = TRUE), NA), -->
<!--             excl8 = ifelse(sum(!is.na(diff)) > 4, -->
<!--                            mean(diff, na.rm = TRUE), NA)) %>%  -->
<!--   gather(exclusion, mean_diff, excl2, excl4, excl8) %>%  -->
<!--   mutate(exclusion = as.numeric(str_replace(exclusion, "excl",""))) -->
<!-- ``` -->


<!-- ```{r,warnings=FALSE,echo=FALSE,message=FALSE} -->

<!-- ds_excl <- ds_excl %>% -->
<!--   group_by(exclusion,lab, age_group, method, nae, subid) %>% -->
<!--   summarise(valid_cases=sum(!is.na(mean_diff)),all_cases=n() -->
<!--             ,percentage=valid_cases/all_cases,d = mean(mean_diff, na.rm = TRUE)) -->

<!-- ds_excl_agg<-ds_excl%>% -->
<!--   group_by(exclusion,lab, age_group, method, nae) %>% -->
<!--   summarise(d_z = mean(d, na.rm = TRUE) / sd(d, na.rm = TRUE),  -->
<!--             n = length(unique(subid)),  -->
<!--             d_z_var = d_var_calc(n, d_z)) %>% -->
<!--   filter(n >= 10) %>% -->
<!--   left_join(ages) %>% -->
<!--   filter(!is.na(d_z))  -->


<!-- ds_excl_2 = ds_excl_agg %>% filter(exclusion==2 & !is.na(d_z))  -->
<!-- intercept_mod_2 <- metafor::rma(d_z ~ 1, vi = d_z_var, slab = lab, data = ds_excl_2, method = "REML") -->

<!-- ds_excl_4 = ds_excl_agg %>% filter(exclusion==4 & !is.na(d_z))  -->
<!-- intercept_mod_4 <- metafor::rma(d_z ~ 1, vi = d_z_var, slab = lab, data = ds_excl_4, method = "REML") -->

<!-- ds_excl_8 = ds_excl_agg %>% filter(exclusion==8 & !is.na(d_z))  -->
<!-- intercept_mod_8 <- metafor::rma(d_z ~ 1, vi = d_z_var, slab = lab, data = ds_excl_8, method = "REML") -->

<!-- ``` -->


<!-- Because our criterion for including infants in the analysis was so liberal (infants needed to contribute data from only two trials to be included), we next conducted an exploration of the effects of different inclusion rules on the results we reported above. In particular, we calculated the meta-analytic effect size with 4 trials and 8 trials as minimum inclusion criteria. For a minimum of 4 trials, the effect size was `r round(intercept_mod_4$b[1], 2)` (CI = [`r round(intercept_mod_4$ci.lb[1], 2)` - `r round(intercept_mod_4$ci.ub[1], 2)`], $z = `r round(intercept_mod_4$z[1], 2)`$, $p `r papaja::printp(intercept_mod_4$pval)`$) and for a minimum of 8 trials the effect size was `r round(intercept_mod_8$b[1], 2)` (CI = [`r round(intercept_mod_8$ci.lb[1], 2)` - `r round(intercept_mod_8$ci.ub[1], 2)`], $z = `r round(intercept_mod_8$z[1], 2)`$, $p `r papaja::printp(intercept_mod_8$pval)`$). In comparison, our original results showed a meta-analytic effect size of -->
<!-- `r round(intercept_mod_2$b[1], 2)` (CI = [`r round(intercept_mod_2$ci.lb[1], 2)` - `r round(intercept_mod_2$ci.ub[1], 2)`], $z = `r round(intercept_mod_2$z[1], 2)`$, $p `r papaja::printp(intercept_mod_2$pval)`$). -->
<!-- Furthermore, we computed effect sizes for each method for each of these additional exclusion criteria (see Table 6). Overall, more stringent inclusion criteria yielded substantially larger effects, although they also led to substantial data loss (especially for eye-tracking labs). -->

<!-- <!-- In addition, all original analyses were repeated with the new exlusion criteria (see APPENDIX). --> -->


<!-- ```{r exclusion_comparison, warning=FALSE,message=FALSE} -->

<!-- coef_exclusion = ds_excl_agg %>%filter(!is.na(d_z)) %>%   -->
<!--   group_by(exclusion,method) %>%  -->
<!--   do(meta_excl = metafor::rma(d_z ~ 1, vi = d_z_var, slab = lab, data = ., method="REML")) %>%  -->
<!--   mutate(estimate=unlist(meta_excl$beta),se=(meta_excl$se))%>% -->
<!--   select(-meta_excl) -->

<!-- ds_excl=ds_excl %>% group_by(exclusion,method) %>% -->
<!--   summarise(valid_cases=sum(valid_cases),all_cases=sum(all_cases) -->
<!--             ,percentage=valid_cases/all_cases) %>% select(-c("all_cases","valid_cases")) -->

<!-- ds_excl=left_join(coef_exclusion,ds_excl,by=c("exclusion","method")) -->


<!-- # prepare Table -->
<!-- ds_excl_est=ds_excl %>% select(method,exclusion,estimate) %>%mutate(est='estimate') %>% -->
<!--   unite(exlest, exclusion, est) %>%  spread(exlest,estimate) -->

<!-- ds_excl_se=ds_excl %>% select(method,exclusion,se) %>%mutate(se_l='se') %>% -->
<!--   unite(exlse, exclusion, se_l) %>%  spread(exlse,se) -->

<!-- ds_excl_per=ds_excl %>% select(method,exclusion,percentage) %>%mutate(per_l='percentage') %>%unite(exlper, exclusion, per_l) %>%  spread(exlper,percentage) -->

<!-- ds_excl=inner_join(ds_excl_est,ds_excl_se,by="method") -->
<!-- ds_excl=inner_join(ds_excl,ds_excl_per,by="method") -->


<!-- ds_excl=ds_excl %>% select(method,`2_estimate`,`2_se`,`2_percentage`,`4_estimate`,`4_se`,`4_percentage`,`8_estimate`,`8_se`,`8_percentage`)  -->


<!-- names(ds_excl)=c("method",rep(c("estimate","SE","%"),3)) -->
<!-- ``` -->

<!-- ```{r exclusiontable, results="asis"} -->
<!-- kable(ds_excl, digits= 2,  -->
<!--       format = "latex",booktabs=T, -->
<!--       longtable = TRUE,  -->
<!--       caption = "Meta-analytic effect size (dz), standard error (SE) and percentage of included participants for three different exclusion criteria") %>%   -->
<!--   add_header_above(c(" " = 1, "2 Trials" = 3, "4 Trials" = 3, "8 Trials" = 3)) %>%  -->
<!--   kable_styling(position = "center") -->

<!-- ``` -->


# Author Contributions




# Conflicts of Interest

The authors declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

# Funding

This work was supported by a grant from Jacobs Foundation to Michael C. Frank and a Global Collaborative Network Grant from Princeton University to Casey Lew-Williams.


# Prior Versions

<!-- Our pre-registered protocol was posted prior to data collection at https://psyarxiv.com/s98ab/. -->

# Disclosures 

## Preregistration

<!-- Our manuscript was reviewed prior to data collection; in addition, we registered our instructions and materials prior to data collection (https://osf.io/gf7vh/). -->

## Data, materials, and online resources

<!-- All materials, data, and analytic code are available at https://osf.io/re95x/; the specific code and data required to render this document are available at https://osf.io/zaewn/.  -->

## Reporting 

<!-- We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study. -->

## Ethical approval

<!-- All labs collected data under their own independent ethical approval via the appropriate governing body for their institution. Central data analyses used exclusively de-identified data. Identifiable video recordings of individual infant participants were coded and archived locally at each lab; where IRB protocols permitted, video recordings were also uploaded to [Databrary](http://databrary.org), a central controlled-access database accessible to other researchers [@databrary].  -->

<!-- Because these videos were not available for all sites, we did not use video information in centralized analyses across labs (e.g., for decision-making regarding exclusions). -->

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
